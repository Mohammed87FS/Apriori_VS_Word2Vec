{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori vs Word2Vec Model Comparison\n",
    "\n",
    "This notebook compares the performance of Apriori and Word2Vec models for detecting complementary products in market basket analysis. The comparison includes:\n",
    "\n",
    "- Execution time and memory usage\n",
    "- Coverage and diversity metrics\n",
    "- Clustering quality \n",
    "- Visualization of key metrics\n",
    "\n",
    "The `ModelEvaluator` class handles the evaluation process, including model training, metric calculation, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlxtend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransactionEncoder\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlxtend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrequent_patterns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apriori, association_rules\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import tracemalloc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File path settings\n",
    "\n",
    "current_dir = os.getcwd() \n",
    "path = os.path.join(current_dir, \"Dataset\")\n",
    "path_results =  os.path.join(current_dir, \"Results\")\n",
    "excel_file = 'df_merged_items_category.xlsx'\n",
    "\n",
    "\n",
    "# Apriori parameters\n",
    "apriori_min_support = 0.01\n",
    "apriori_min_confidence = 0.05\n",
    "apriori_min_lift = 1.2\n",
    "\n",
    "# Word2Vec parameters\n",
    "vector_size_num = 100\n",
    "window_num = 5\n",
    "workers_num = 4\n",
    "negative_sampling = 10\n",
    "epoches_num = 10\n",
    "min_count = 2\n",
    "w2v_min_count = 2\n",
    "w2v_topn = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelEvaluator Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'apriori_min_support' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mModelEvaluator\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;43;03m    Class to evaluate and compare Apriori and Word2Vec models across multiple metrics.\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_subset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 50\u001b[0m, in \u001b[0;36mModelEvaluator\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword2vec_complementary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset loaded for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasket)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m transactions and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_products)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unique products\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_apriori\u001b[39m(\u001b[38;5;28mself\u001b[39m, min_support\u001b[38;5;241m=\u001b[39m\u001b[43mapriori_min_support\u001b[49m, min_confidence\u001b[38;5;241m=\u001b[39mapriori_min_confidence, min_lift\u001b[38;5;241m=\u001b[39mapriori_min_lift):\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    Run Apriori algorithm and measure performance.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        dict: Performance metrics for Apriori\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Apriori algorithm for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'apriori_min_support' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Class to evaluate and compare Apriori and Word2Vec models across multiple metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, path_results, excel_file, data_subset=None, subset_name=\"whole\"):\n",
    "        \"\"\"\n",
    "        Initialize with dataset path and optional data subset.\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the data directory\n",
    "            path_results (str): Path to save results\n",
    "            excel_file (str): Name of the Excel file containing transaction data\n",
    "            data_subset (DataFrame, optional): Subset of data to use instead of loading from file\n",
    "            subset_name (str): Name of the data subset (e.g., \"whole\", \"first_third\", \"last_two_thirds\")\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.path_results = path_results\n",
    "        self.excel_file = excel_file\n",
    "        self.file_path = os.path.join(data_path, excel_file)\n",
    "        self.subset_name = subset_name\n",
    "        self.metrics = {\n",
    "            'apriori': {},\n",
    "            'word2vec': {}\n",
    "        }\n",
    "        \n",
    "        if data_subset is not None:\n",
    "            self.data_excel = data_subset.copy()\n",
    "        else:\n",
    "            self.data_excel = pd.read_excel(self.file_path)\n",
    "        \n",
    "        self.data_excel.dropna(subset=['Itemname'], inplace=True)\n",
    "        \n",
    "        self.basket = self.data_excel.groupby('BillNo')['Itemname'].apply(list)\n",
    "        self.transactions = self.basket.values.tolist()\n",
    "        self.unique_products = self.data_excel['Itemname'].unique().tolist()\n",
    "    \n",
    "        self.product_to_category = {}\n",
    "        if 'category' in self.data_excel.columns:\n",
    "            \n",
    "            product_category_mapping = self.data_excel[['Itemname', 'category']].drop_duplicates()\n",
    "            self.product_to_category = dict(zip(product_category_mapping['Itemname'], product_category_mapping['category']))\n",
    "        \n",
    "      \n",
    "        self.apriori_rules = None\n",
    "        self.word2vec_complementary = None\n",
    "        \n",
    "        print(f\"Dataset loaded for {subset_name} with {len(self.basket)} transactions and {len(self.unique_products)} unique products\")\n",
    "        \n",
    "    def run_apriori(self, min_support=apriori_min_support, min_confidence=apriori_min_confidence, min_lift=apriori_min_lift):\n",
    "        \"\"\"\n",
    "        Run Apriori algorithm and measure performance.\n",
    "        \n",
    "        Args:\n",
    "            min_support (float): Minimum support threshold\n",
    "            min_confidence (float): Minimum confidence threshold\n",
    "            min_lift (float): Minimum lift threshold\n",
    "            \n",
    "        Returns:\n",
    "            dict: Performance metrics for Apriori\n",
    "        \"\"\"\n",
    "        print(f\"Running Apriori algorithm for {self.subset_name}...\")\n",
    "        \n",
    "       \n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(self.transactions).transform(self.transactions)\n",
    "        basket_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        \n",
    "       \n",
    "        tracemalloc.start()\n",
    "        start_memory = tracemalloc.get_traced_memory()[1] \n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f\"Apriori started with {start_memory} bytes memory used\")\n",
    "\n",
    "      \n",
    "        frequent_itemsets = apriori(basket_encoded, min_support=min_support, use_colnames=True)\n",
    "\n",
    "     \n",
    "        rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=min_lift)\n",
    "        rules = rules[rules['confidence'] >= min_confidence]\n",
    "\n",
    "     \n",
    "        rules[\"antecedent_len\"] = rules[\"antecedents\"].apply(lambda x: len(x))\n",
    "\n",
    "       \n",
    "        end_time = time.time()\n",
    "        current_memory = tracemalloc.get_traced_memory()[1]\n",
    "        peak_memory = (current_memory - start_memory) / (1024 * 1024) \n",
    "        tracemalloc.stop()\n",
    "        print(f\"Apriori ended with {peak_memory} MB memory used\")\n",
    "        \n",
    "        self.apriori_rules = rules\n",
    "        \n",
    "       \n",
    "        self.apriori_product_pairs = self.convert_apriori_rules_to_pairs(rules)\n",
    "        \n",
    "        # Store metrics\n",
    "        self.metrics['apriori'] = {\n",
    "            'execution_time': end_time - start_time,\n",
    "            'memory_usage': peak_memory,\n",
    "            'num_rules': len(rules),\n",
    "            'coverage': self.calculate_apriori_coverage(rules),\n",
    "            'diversity': self.calculate_apriori_diversity(rules),\n",
    "        }\n",
    "        \n",
    "        print(f\"Apriori completed for {self.subset_name} with {len(rules)} rules generated\")\n",
    "        return self.metrics['apriori']\n",
    "    \n",
    "    def convert_apriori_rules_to_pairs(self, apriori_rules):\n",
    "        \"\"\"\n",
    "        Convert Apriori rules to a consistent product pairs table with category information.\n",
    "        \n",
    "        Args:\n",
    "            apriori_rules (DataFrame): DataFrame containing Apriori rules\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Consistent format with one product pair per row and category information\n",
    "        \"\"\"\n",
    "        product_pairs = []\n",
    "        \n",
    "        for _, rule in apriori_rules.iterrows():\n",
    "          \n",
    "            antecedents = list(rule['antecedents'])\n",
    "            consequents = list(rule['consequents'])\n",
    "            \n",
    "       \n",
    "            for antecedent in antecedents:\n",
    "                for consequent in consequents:\n",
    "                 \n",
    "                    if antecedent == consequent:\n",
    "                        continue\n",
    "           \n",
    "                    pair = {\n",
    "                        'Original Product': antecedent,\n",
    "                        'Complementary Product': consequent,\n",
    "                        'Confidence': rule['confidence'],\n",
    "                        'Lift': rule['lift'],\n",
    "                        'Support': rule['support']\n",
    "                    }\n",
    "                    \n",
    "                    if self.product_to_category:\n",
    "                        if antecedent in self.product_to_category:\n",
    "                            pair['Original Product category'] = self.product_to_category[antecedent]\n",
    "                        else:\n",
    "                            pair['Original Product category'] = 'Unknown'\n",
    "                            \n",
    "                        if consequent in self.product_to_category:\n",
    "                            pair['Complementary Product category'] = self.product_to_category[consequent]\n",
    "                        else:\n",
    "                            pair['Complementary Product category'] = 'Unknown'\n",
    "                    \n",
    "                    product_pairs.append(pair)\n",
    "        \n",
    "        return pd.DataFrame(product_pairs)\n",
    "\n",
    "    def run_word2vec(self, vector_size=vector_size_num, window=window_num, min_count=min_count, topn=w2v_topn):\n",
    "        \"\"\"\n",
    "        Run Word2Vec model and measure performance.\n",
    "        \n",
    "        Args:\n",
    "            vector_size (int): Dimensionality of word vectors\n",
    "            window (int): Context window size\n",
    "            min_count (int): Minimum word frequency\n",
    "            topn (int): Number of top complementary products to generate\n",
    "            \n",
    "        Returns:\n",
    "            dict: Performance metrics for Word2Vec\n",
    "        \"\"\"\n",
    "        print(f\"Running Word2Vec model for {self.subset_name}...\")\n",
    "        \n",
    "   \n",
    "        tracemalloc.start()\n",
    "        start_memory = tracemalloc.get_traced_memory()[1] \n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f\"Word2Vec started with {start_memory} bytes memory used\")\n",
    "\n",
    "        model = Word2Vec(\n",
    "            sentences=self.transactions,\n",
    "            vector_size=vector_size_num,\n",
    "            window=window_num,\n",
    "            sg=1,  \n",
    "            negative=negative_sampling, \n",
    "            min_count=min_count,\n",
    "            workers=workers_num,\n",
    "            epochs=epoches_num\n",
    "        )\n",
    "        \n",
    "       \n",
    "        comprehensive_results = []\n",
    "        \n",
    "        model_vocab = set(model.wv.index_to_key)\n",
    "        \n",
    "        for product in self.unique_products:\n",
    "            if product in model_vocab:\n",
    "                try:\n",
    "                    similar_products = model.wv.most_similar(product, topn=topn)\n",
    "                    \n",
    "                    complementary_list = [\n",
    "                        {\n",
    "                            'Original Product': product,\n",
    "                            'Complementary Product': comp_product,\n",
    "                            'Similarity Score': similarity,\n",
    "                            'Rank': rank + 1\n",
    "                        }\n",
    "                        for rank, (comp_product, similarity) in enumerate(similar_products)\n",
    "                    ]\n",
    "                    \n",
    "                    comprehensive_results.extend(complementary_list)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        complementary_products_df = pd.DataFrame(comprehensive_results)\n",
    "        \n",
    "        if self.product_to_category and not complementary_products_df.empty:\n",
    "            \n",
    "            complementary_products_df['Original Product category'] = complementary_products_df['Original Product'].apply(\n",
    "                lambda x: self.product_to_category.get(x, 'Unknown')\n",
    "            )\n",
    "     \n",
    "            complementary_products_df['Complementary Product category'] = complementary_products_df['Complementary Product'].apply(\n",
    "                lambda x: self.product_to_category.get(x, 'Unknown')\n",
    "            )\n",
    "        \n",
    "   \n",
    "        end_time = time.time()\n",
    "        current_memory = tracemalloc.get_traced_memory()[1]\n",
    "        peak_memory = (current_memory - start_memory) / (1024 * 1024) \n",
    "        tracemalloc.stop()\n",
    "        print(f\"Word2Vec ended with {peak_memory} MB memory used\")\n",
    "        \n",
    "        self.word2vec_complementary = complementary_products_df\n",
    "        self.word2vec_model = model\n",
    "        \n",
    "       \n",
    "        self.metrics['word2vec'] = {\n",
    "            'execution_time': end_time - start_time,\n",
    "            'memory_usage': peak_memory,\n",
    "            'num_recommendations': len(complementary_products_df),\n",
    "            'coverage': self.calculate_word2vec_coverage(model, complementary_products_df),\n",
    "            'diversity': self.calculate_word2vec_diversity(complementary_products_df),\n",
    "        }\n",
    "        \n",
    "        print(f\"Word2Vec completed for {self.subset_name} with {len(complementary_products_df)} recommendations generated\")\n",
    "        return self.metrics['word2vec']\n",
    "    \n",
    "    def calculate_apriori_coverage(self, rules):\n",
    "        \"\"\"\n",
    "        Calculate coverage for Apriori rules.\n",
    "        \n",
    "        Args:\n",
    "            rules (DataFrame): Association rules\n",
    "            \n",
    "        Returns:\n",
    "            float: Coverage score\n",
    "        \"\"\"\n",
    "        unique_antecedents = set()\n",
    "        unique_consequents = set()\n",
    "        \n",
    "        for _, row in rules.iterrows():\n",
    "            antecedents = set(row['antecedents'])\n",
    "            consequents = set(row['consequents'])\n",
    "            \n",
    "            unique_antecedents.update(antecedents)\n",
    "            unique_consequents.update(consequents)\n",
    "        \n",
    "        unique_items = unique_antecedents.union(unique_consequents)\n",
    "        coverage = len(unique_items) / len(self.unique_products)\n",
    "        \n",
    "        return coverage\n",
    "    \n",
    "    def calculate_word2vec_coverage(self, model, complementary_df):\n",
    "        \"\"\"\n",
    "        Calculate coverage for Word2Vec model.\n",
    "        \n",
    "        Args:\n",
    "            model (Word2Vec): Trained Word2Vec model\n",
    "            complementary_df (DataFrame): Generated complementary products\n",
    "            \n",
    "        Returns:\n",
    "            float: Coverage score\n",
    "        \"\"\"\n",
    "        products_with_embeddings = complementary_df['Original Product'].nunique()\n",
    "        coverage = products_with_embeddings / len(self.unique_products)\n",
    "        \n",
    "        return coverage\n",
    "    \n",
    "    def calculate_apriori_diversity(self, rules):\n",
    "        \"\"\"\n",
    "        Calculate diversity for Apriori rules.\n",
    "        \n",
    "        Args:\n",
    "            rules (DataFrame): Association rules\n",
    "            \n",
    "        Returns:\n",
    "            float: Diversity score\n",
    "        \"\"\"\n",
    "        antecedent_consequents = {}\n",
    "        \n",
    "        for _, row in rules.iterrows():\n",
    "            antecedent = frozenset(row['antecedents'])\n",
    "            consequent = frozenset(row['consequents'])\n",
    "            \n",
    "            if antecedent in antecedent_consequents:\n",
    "                antecedent_consequents[antecedent].add(consequent)\n",
    "            else:\n",
    "                antecedent_consequents[antecedent] = {consequent}\n",
    "        \n",
    "      \n",
    "        if len(antecedent_consequents) > 0:\n",
    "            diversity = sum(len(consequents) for consequents in antecedent_consequents.values()) / len(antecedent_consequents)\n",
    "        else:\n",
    "            diversity = 0\n",
    "        \n",
    "        return diversity\n",
    "    \n",
    "    def calculate_word2vec_diversity(self, complementary_df):\n",
    "        \"\"\"\n",
    "        Calculate diversity for Word2Vec recommendations.\n",
    "        \n",
    "        Args:\n",
    "            complementary_df (DataFrame): Generated complementary products\n",
    "            \n",
    "        Returns:\n",
    "            float: Diversity score\n",
    "        \"\"\"\n",
    "        total_recommendations = len(complementary_df)\n",
    "        unique_recommendations = complementary_df['Complementary Product'].nunique()\n",
    "        \n",
    "        if total_recommendations > 0:\n",
    "            diversity = unique_recommendations / total_recommendations\n",
    "        else:\n",
    "            diversity = 0\n",
    "        \n",
    "        return diversity\n",
    "    \n",
    "    def analyze_category_relationships(self):\n",
    "        \"\"\"\n",
    "        Analyze the Cross-Category Complementarity.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Analysis results for both methods\n",
    "        \"\"\"\n",
    "        results = {'apriori': {}, 'word2vec': {}}\n",
    "        \n",
    "    \n",
    "        if self.product_to_category and self.apriori_product_pairs is not None and self.word2vec_complementary is not None:\n",
    "            \n",
    "            if 'Original Product category' in self.apriori_product_pairs.columns:\n",
    "               \n",
    "                category_combinations = self.apriori_product_pairs.groupby(\n",
    "                    ['Original Product category', 'Complementary Product category']\n",
    "                ).size().reset_index(name='frequency')\n",
    "                \n",
    "          \n",
    "                category_combinations = category_combinations.sort_values('frequency', ascending=False)\n",
    "                \n",
    "                \n",
    "                same_category = self.apriori_product_pairs[\n",
    "                    self.apriori_product_pairs['Original Product category'] == \n",
    "                    self.apriori_product_pairs['Complementary Product category']\n",
    "                ]\n",
    "                same_category_percentage = len(same_category) / len(self.apriori_product_pairs) * 100\n",
    "                \n",
    "                results['apriori'] = {\n",
    "                    'top_category_combinations': category_combinations.head(10),\n",
    "                    'same_category_percentage': same_category_percentage,\n",
    "                    'cross_category_percentage': 100 - same_category_percentage\n",
    "                }\n",
    "            \n",
    "     \n",
    "            if 'Original Product category' in self.word2vec_complementary.columns:\n",
    "               \n",
    "                category_combinations = self.word2vec_complementary.groupby(\n",
    "                    ['Original Product category', 'Complementary Product category']\n",
    "                ).size().reset_index(name='frequency')\n",
    "                \n",
    "              \n",
    "                category_combinations = category_combinations.sort_values('frequency', ascending=False)\n",
    "                \n",
    "               \n",
    "                same_category = self.word2vec_complementary[\n",
    "                    self.word2vec_complementary['Original Product category'] == \n",
    "                    self.word2vec_complementary['Complementary Product category']\n",
    "                ]\n",
    "                same_category_percentage = len(same_category) / len(self.word2vec_complementary) * 100\n",
    "                \n",
    "                results['word2vec'] = {\n",
    "                    'top_category_combinations': category_combinations.head(10),\n",
    "                    'same_category_percentage': same_category_percentage,\n",
    "                    'cross_category_percentage': 100 - same_category_percentage\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"\n",
    "        Compare performance metrics between Apriori and Word2Vec.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: Comparison of metrics\n",
    "        \"\"\"\n",
    "     \n",
    "        if not self.metrics['apriori'] or not self.metrics['word2vec']:\n",
    "            print(\"Both models need to be run before comparison\")\n",
    "            return\n",
    "        \n",
    "      \n",
    "        comparison = pd.DataFrame({\n",
    "            'Metric': [\n",
    "                'Execution Time (s)',\n",
    "                'Memory Usage (MB)',\n",
    "                'Number of Rules/Recommendations',\n",
    "                'Coverage',\n",
    "                'Diversity'\n",
    "            ],\n",
    "            'Apriori': [\n",
    "                self.metrics['apriori']['execution_time'],\n",
    "                self.metrics['apriori']['memory_usage'],\n",
    "                self.metrics['apriori']['num_rules'],\n",
    "                self.metrics['apriori']['coverage'],\n",
    "                self.metrics['apriori']['diversity']\n",
    "            ],\n",
    "            'Word2Vec': [\n",
    "                self.metrics['word2vec']['execution_time'],\n",
    "                self.metrics['word2vec']['memory_usage'],\n",
    "                self.metrics['word2vec']['num_recommendations'],\n",
    "                self.metrics['word2vec']['coverage'],\n",
    "                self.metrics['word2vec']['diversity']\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def visualize_comparison(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize comparison metrics between Apriori and Word2Vec.\n",
    "        \n",
    "        Args:\n",
    "            save_path (str, optional): Path to save the visualization\n",
    "        \"\"\"\n",
    "        comparison = self.compare_models()\n",
    "        \n",
    "    \n",
    "        metrics_to_plot = ['Execution Time (s)', 'Memory Usage (MB)', 'Coverage', 'Diversity']\n",
    "     \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            data = comparison[comparison['Metric'] == metric]\n",
    "            values = data.iloc[0, 1:].astype(float)\n",
    "            \n",
    "    \n",
    "            ax = axes[i]\n",
    "            sns.barplot(x=['Apriori', 'Word2Vec'], y=values, ax=ax, palette='viridis')\n",
    "            ax.set_title(f\"{self.subset_name} - {metric}\", fontsize=14)\n",
    "            ax.set_ylabel('Value', fontsize=12)\n",
    "            ax.set_xlabel('')\n",
    "            \n",
    "         \n",
    "            for j, v in enumerate(values):\n",
    "                ax.text(j, v, f'{v:.4f}', ha='center', va='bottom', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_category_relationships(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize category relationships for both methods.\n",
    "        \n",
    "        Args:\n",
    "            save_path (str, optional): Path to save the visualization\n",
    "        \"\"\"\n",
    "        \n",
    "        category_analysis = self.analyze_category_relationships()\n",
    "        \n",
    "        if not category_analysis['apriori'] or not category_analysis['word2vec']:\n",
    "            print(\"Category analysis is not available. Make sure models have been run and category information exists.\")\n",
    "            return\n",
    "        \n",
    " \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "        \n",
    "    \n",
    "        axes[0].bar(['Same category', 'Cross category'], \n",
    "                 [category_analysis['apriori']['same_category_percentage'], \n",
    "                  category_analysis['apriori']['cross_category_percentage']])\n",
    "        axes[0].set_title(f'{self.subset_name} - Apriori: Category Relationships', fontsize=14)\n",
    "        axes[0].set_ylabel('Percentage (%)', fontsize=12)\n",
    "        \n",
    "     \n",
    "        for i, v in enumerate([category_analysis['apriori']['same_category_percentage'], \n",
    "                               category_analysis['apriori']['cross_category_percentage']]):\n",
    "            axes[0].text(i, v, f'{v:.1f}%', ha='center', va='bottom', fontsize=12)\n",
    "        \n",
    "      \n",
    "        axes[1].bar(['Same category', 'Cross category'], \n",
    "                 [category_analysis['word2vec']['same_category_percentage'], \n",
    "                  category_analysis['word2vec']['cross_category_percentage']])\n",
    "        axes[1].set_title(f'{self.subset_name} - Word2Vec: Category Relationships', fontsize=14)\n",
    "        axes[1].set_ylabel('Percentage (%)', fontsize=12)\n",
    "        \n",
    "    \n",
    "        for i, v in enumerate([category_analysis['word2vec']['same_category_percentage'], \n",
    "                               category_analysis['word2vec']['cross_category_percentage']]):\n",
    "            axes[1].text(i, v, f'{v:.1f}%', ha='center', va='bottom', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def save_results(self, output_dir=None):\n",
    "        \"\"\"\n",
    "        Save evaluation results to files.\n",
    "        \n",
    "        Args:\n",
    "            output_dir (str, optional): Directory to save results\n",
    "        \"\"\"\n",
    "        if output_dir is None:\n",
    "            output_dir = self.path_results\n",
    "            \n",
    "    \n",
    "        subset_dir = os.path.join(output_dir, self.subset_name)\n",
    "        os.makedirs(subset_dir, exist_ok=True)\n",
    "        \n",
    "      \n",
    "        comparison = self.compare_models()\n",
    "        comparison.to_csv(os.path.join(subset_dir, f'{self.subset_name}_model_comparison_metrics.csv'), index=False)\n",
    "        \n",
    "    \n",
    "        if self.apriori_rules is not None:\n",
    "            self.apriori_rules.to_excel(os.path.join(subset_dir, f'{self.subset_name}_apriori_rules.xlsx'), index=False)\n",
    "   \n",
    "        if hasattr(self, 'apriori_product_pairs') and self.apriori_product_pairs is not None:\n",
    "            self.apriori_product_pairs.to_excel(os.path.join(subset_dir, f'{self.subset_name}_apriori_product_pairs_with_categories.xlsx'), index=False)\n",
    "        \n",
    "\n",
    "        if self.word2vec_complementary is not None:\n",
    "            self.word2vec_complementary.to_excel(os.path.join(subset_dir, f'{self.subset_name}_word2vec_recommendations.xlsx'), index=False)\n",
    "   \n",
    "        self.visualize_comparison(save_path=os.path.join(subset_dir, f'{self.subset_name}_metric_comparison.png'))\n",
    "   \n",
    "        if hasattr(self, 'product_to_category') and self.product_to_category:\n",
    "            self.visualize_category_relationships(save_path=os.path.join(subset_dir, f'{self.subset_name}_category_relationships.png'))\n",
    "            \n",
    "     \n",
    "            category_analysis = self.analyze_category_relationships()\n",
    "    \n",
    "            if 'top_category_combinations' in category_analysis['apriori']:\n",
    "                category_analysis['apriori']['top_category_combinations'].to_excel(\n",
    "                    os.path.join(subset_dir, f'{self.subset_name}_apriori_top_category_combinations.xlsx'), index=False\n",
    "                )\n",
    "            \n",
    "\n",
    "            if 'top_category_combinations' in category_analysis['word2vec']:\n",
    "                category_analysis['word2vec']['top_category_combinations'].to_excel(\n",
    "                    os.path.join(subset_dir, f'{self.subset_name}_word2vec_top_category_combinations.xlsx'), index=False\n",
    "                )\n",
    "        \n",
    "        print(f\"Results for {self.subset_name} saved to {subset_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m full_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mpath\u001b[49m, excel_file))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ===== DATA PARTITIONING =====\u001b[39;00m\n\u001b[0;32m      5\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "full_data = pd.read_excel(os.path.join(path, excel_file))\n",
    "    \n",
    "# ===== DATA PARTITIONING =====\n",
    "\n",
    "n = len(full_data)\n",
    "one_third_index = n // 3\n",
    "\n",
    "df_whole = full_data.copy()\n",
    "df_first_third = full_data.iloc[:one_third_index]\n",
    "df_last_two_thirds = full_data.iloc[one_third_index:]\n",
    "\n",
    "print(f\"Full dataset: {len(df_whole)} records\")\n",
    "print(f\"First third: {len(df_first_third)} records\")\n",
    "print(f\"Last two thirds: {len(df_last_two_thirds)} records\")\n",
    "\n",
    "\n",
    "os.makedirs(path_results, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Algorithms on Data-Portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EVALUATING WHOLE DATASET =====\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ModelEvaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== EVALUATING WHOLE DATASET =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m evaluator_whole \u001b[38;5;241m=\u001b[39m \u001b[43mModelEvaluator\u001b[49m(path, path_results, excel_file, df_whole, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhole_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m evaluator_whole\u001b[38;5;241m.\u001b[39mrun_apriori()\n\u001b[0;32m      4\u001b[0m evaluator_whole\u001b[38;5;241m.\u001b[39mrun_word2vec()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ModelEvaluator' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== EVALUATING WHOLE DATASET =====\")\n",
    "evaluator_whole = ModelEvaluator(path, path_results, excel_file, df_whole, \"whole_dataset\")\n",
    "evaluator_whole.run_apriori()\n",
    "evaluator_whole.run_word2vec()\n",
    "evaluator_whole.save_results()\n",
    "\n",
    "# ===== EVALUATE FIRST THIRD =====\n",
    "print(\"\\n===== EVALUATING FIRST THIRD OF DATASET =====\")\n",
    "evaluator_first_third = ModelEvaluator(path, path_results, excel_file, df_first_third, \"first_third\")\n",
    "evaluator_first_third.run_apriori()\n",
    "evaluator_first_third.run_word2vec()\n",
    "evaluator_first_third.save_results()\n",
    "\n",
    "# ===== EVALUATE LAST TWO THIRDS =====\n",
    "print(\"\\n===== EVALUATING LAST TWO THIRDS OF DATASET =====\")\n",
    "evaluator_last_two_thirds = ModelEvaluator(path, path_results, excel_file, df_last_two_thirds, \"last_two_thirds\")\n",
    "evaluator_last_two_thirds.run_apriori()\n",
    "evaluator_last_two_thirds.run_word2vec()\n",
    "evaluator_last_two_thirds.save_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluator_whole' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Create combined comparison\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[43mcreate_combined_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll evaluations completed and results saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_combined_category_comparison\u001b[39m():\n",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m, in \u001b[0;36mcreate_combined_comparison\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m apriori_values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mevaluator_whole\u001b[49m\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapriori\u001b[39m\u001b[38;5;124m'\u001b[39m][metric],\n\u001b[0;32m     14\u001b[0m     evaluator_first_third\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapriori\u001b[39m\u001b[38;5;124m'\u001b[39m][metric],\n\u001b[0;32m     15\u001b[0m     evaluator_last_two_thirds\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapriori\u001b[39m\u001b[38;5;124m'\u001b[39m][metric]\n\u001b[0;32m     16\u001b[0m ]\n\u001b[0;32m     18\u001b[0m word2vec_values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     19\u001b[0m     evaluator_whole\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2vec\u001b[39m\u001b[38;5;124m'\u001b[39m][metric],\n\u001b[0;32m     20\u001b[0m     evaluator_first_third\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2vec\u001b[39m\u001b[38;5;124m'\u001b[39m][metric],\n\u001b[0;32m     21\u001b[0m     evaluator_last_two_thirds\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2vec\u001b[39m\u001b[38;5;124m'\u001b[39m][metric]\n\u001b[0;32m     22\u001b[0m ]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Set bar positions\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluator_whole' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+AAAAH/CAYAAADXOLcaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAh80lEQVR4nO3df2zV9b348VcLttXMVrxcyo9bx9Vd5zYVHEhXHTHedDaZYZc/btaLCxCi87pxjdrsTvAHnXOj3E0NyRVHZO665MYLG5neZZB6Xa9k2bU3ZPxINBcwjjGIWQvcXVqGG5X28/1jWfftKMgp9AXI45GcP/r2/T7nfcybhief86OsKIoiAAAAgFFVfrY3AAAAABcCAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAlKDvCf/OQnMWfOnJg8eXKUlZXFSy+99J5rNm3aFB//+MejsrIyPvShD8Xzzz8/gq0CAADA+avkAD9y5EhMmzYtVq1adUrzf/GLX8Ttt98et956a2zfvj3uv//+uOuuu+Lll18uebMAAABwvioriqIY8eKysnjxxRdj7ty5J5zz4IMPxoYNG+KNN94YHPu7v/u7OHToULS3t4/0oQEAAOC8Mna0H6CzszMaGxuHjDU1NcX9999/wjVHjx6No0ePDv48MDAQv/71r+PP/uzPoqysbLS2CgAAABERURRFHD58OCZPnhzl5Wfm49NGPcC7urqitrZ2yFhtbW309vbGb3/727j44ouPW9PW1haPPfbYaG8NAAAATmrfvn3xF3/xF2fkvkY9wEdi6dKl0dLSMvhzT09PXHHFFbFv376orq4+izsDAADgQtDb2xt1dXVx6aWXnrH7HPUAnzhxYnR3dw8Z6+7ujurq6mGvfkdEVFZWRmVl5XHj1dXVAhwAAIA0Z/Jt0KP+PeANDQ3R0dExZOyVV16JhoaG0X5oAAAAOGeUHOC/+c1vYvv27bF9+/aI+P3XjG3fvj327t0bEb9/+fiCBQsG599zzz2xe/fu+PKXvxw7d+6MZ555Jr73ve/FAw88cGaeAQAAAJwHSg7wn/3sZ3HDDTfEDTfcEBERLS0tccMNN8SyZcsiIuJXv/rVYIxHRPzlX/5lbNiwIV555ZWYNm1aPPnkk/Htb387mpqaztBTAAAAgHPfaX0PeJbe3t6oqamJnp4e7wEHAABg1I1Gh476e8ABAAAAAQ4AAAApBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQIIRBfiqVati6tSpUVVVFfX19bF58+aTzl+5cmV8+MMfjosvvjjq6urigQceiN/97ncj2jAAAACcj0oO8HXr1kVLS0u0trbG1q1bY9q0adHU1BT79+8fdv4LL7wQS5YsidbW1tixY0c899xzsW7dunjooYdOe/MAAABwvig5wJ966qn4/Oc/H4sWLYqPfvSjsXr16rjkkkviO9/5zrDzX3vttbj55pvjjjvuiKlTp8Ztt90W8+bNe8+r5gAAAPB+UlKA9/X1xZYtW6KxsfGPd1BeHo2NjdHZ2Tnsmptuuim2bNkyGNy7d++OjRs3xqc//enT2DYAAACcX8aWMvngwYPR398ftbW1Q8Zra2tj586dw66544474uDBg/HJT34yiqKIY8eOxT333HPSl6AfPXo0jh49Ovhzb29vKdsEAACAc86ofwr6pk2bYvny5fHMM8/E1q1b4wc/+EFs2LAhHn/88ROuaWtri5qamsFbXV3daG8TAAAARlVZURTFqU7u6+uLSy65JNavXx9z584dHF+4cGEcOnQo/v3f//24NbNnz45PfOIT8c1vfnNw7F//9V/j7rvvjt/85jdRXn78vwEMdwW8rq4uenp6orq6+lS3CwAAACPS29sbNTU1Z7RDS7oCXlFRETNmzIiOjo7BsYGBgejo6IiGhoZh17zzzjvHRfaYMWMiIuJE7V9ZWRnV1dVDbgAAAHA+K+k94BERLS0tsXDhwpg5c2bMmjUrVq5cGUeOHIlFixZFRMSCBQtiypQp0dbWFhERc+bMiaeeeipuuOGGqK+vj7feeiseffTRmDNnzmCIAwAAwPtdyQHe3NwcBw4ciGXLlkVXV1dMnz492tvbBz+Ybe/evUOueD/yyCNRVlYWjzzySLz99tvx53/+5zFnzpz4+te/fuaeBQAAAJzjSnoP+NkyGq+9BwAAgBM56+8BBwAAAEZGgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQIIRBfiqVati6tSpUVVVFfX19bF58+aTzj906FAsXrw4Jk2aFJWVlXH11VfHxo0bR7RhAAAAOB+NLXXBunXroqWlJVavXh319fWxcuXKaGpqil27dsWECROOm9/X1xef+tSnYsKECbF+/fqYMmVK/PKXv4zLLrvsTOwfAAAAzgtlRVEUpSyor6+PG2+8MZ5++umIiBgYGIi6urq49957Y8mSJcfNX716dXzzm9+MnTt3xkUXXTSiTfb29kZNTU309PREdXX1iO4DAAAATtVodGhJL0Hv6+uLLVu2RGNj4x/voLw8Ghsbo7Ozc9g1P/zhD6OhoSEWL14ctbW1ce2118by5cujv7//hI9z9OjR6O3tHXIDAACA81lJAX7w4MHo7++P2traIeO1tbXR1dU17Jrdu3fH+vXro7+/PzZu3BiPPvpoPPnkk/G1r33thI/T1tYWNTU1g7e6urpStgkAAADnnFH/FPSBgYGYMGFCPPvsszFjxoxobm6Ohx9+OFavXn3CNUuXLo2enp7B2759+0Z7mwAAADCqSvoQtvHjx8eYMWOiu7t7yHh3d3dMnDhx2DWTJk2Kiy66KMaMGTM49pGPfCS6urqir68vKioqjltTWVkZlZWVpWwNAAAAzmklXQGvqKiIGTNmREdHx+DYwMBAdHR0RENDw7Brbr755njrrbdiYGBgcOzNN9+MSZMmDRvfAAAA8H5U8kvQW1paYs2aNfHd7343duzYEV/4whfiyJEjsWjRooiIWLBgQSxdunRw/he+8IX49a9/Hffdd1+8+eabsWHDhli+fHksXrz4zD0LAAAAOMeV/D3gzc3NceDAgVi2bFl0dXXF9OnTo729ffCD2fbu3Rvl5X/s+rq6unj55ZfjgQceiOuvvz6mTJkS9913Xzz44INn7lkAAADAOa7k7wE/G3wPOAAAAJnO+veAAwAAACMjwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASDCiAF+1alVMnTo1qqqqor6+PjZv3nxK69auXRtlZWUxd+7ckTwsAAAAnLdKDvB169ZFS0tLtLa2xtatW2PatGnR1NQU+/fvP+m6PXv2xJe+9KWYPXv2iDcLAAAA56uSA/ypp56Kz3/+87Fo0aL46Ec/GqtXr45LLrkkvvOd75xwTX9/f3zuc5+Lxx57LK688srT2jAAAACcj0oK8L6+vtiyZUs0Njb+8Q7Ky6OxsTE6OztPuO6rX/1qTJgwIe68885TepyjR49Gb2/vkBsAAACcz0oK8IMHD0Z/f3/U1tYOGa+trY2urq5h1/z0pz+N5557LtasWXPKj9PW1hY1NTWDt7q6ulK2CQAAAOecUf0U9MOHD8f8+fNjzZo1MX78+FNet3Tp0ujp6Rm87du3bxR3CQAAAKNvbCmTx48fH2PGjInu7u4h493d3TFx4sTj5v/85z+PPXv2xJw5cwbHBgYGfv/AY8fGrl274qqrrjpuXWVlZVRWVpayNQAAADinlXQFvKKiImbMmBEdHR2DYwMDA9HR0RENDQ3Hzb/mmmvi9ddfj+3btw/ePvOZz8Stt94a27dv99JyAAAALhglXQGPiGhpaYmFCxfGzJkzY9asWbFy5co4cuRILFq0KCIiFixYEFOmTIm2traoqqqKa6+9dsj6yy67LCLiuHEAAAB4Pys5wJubm+PAgQOxbNmy6OrqiunTp0d7e/vgB7Pt3bs3ystH9a3lAAAAcN4pK4qiONubeC+9vb1RU1MTPT09UV1dfba3AwAAwPvcaHSoS9UAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAECCEQX4qlWrYurUqVFVVRX19fWxefPmE85ds2ZNzJ49O8aNGxfjxo2LxsbGk84HAACA96OSA3zdunXR0tISra2tsXXr1pg2bVo0NTXF/v37h52/adOmmDdvXrz66qvR2dkZdXV1cdttt8Xbb7992psHAACA80VZURRFKQvq6+vjxhtvjKeffjoiIgYGBqKuri7uvffeWLJkyXuu7+/vj3HjxsXTTz8dCxYsOKXH7O3tjZqamujp6Ynq6upStgsAAAAlG40OLekKeF9fX2zZsiUaGxv/eAfl5dHY2BidnZ2ndB/vvPNOvPvuu3H55ZefcM7Ro0ejt7d3yA0AAADOZyUF+MGDB6O/vz9qa2uHjNfW1kZXV9cp3ceDDz4YkydPHhLxf6qtrS1qamoGb3V1daVsEwAAAM45qZ+CvmLFili7dm28+OKLUVVVdcJ5S5cujZ6ensHbvn37EncJAAAAZ97YUiaPHz8+xowZE93d3UPGu7u7Y+LEiSdd+8QTT8SKFSvixz/+cVx//fUnnVtZWRmVlZWlbA0AAADOaSVdAa+oqIgZM2ZER0fH4NjAwEB0dHREQ0PDCdd94xvfiMcffzza29tj5syZI98tAAAAnKdKugIeEdHS0hILFy6MmTNnxqxZs2LlypVx5MiRWLRoUURELFiwIKZMmRJtbW0REfFP//RPsWzZsnjhhRdi6tSpg+8V/8AHPhAf+MAHzuBTAQAAgHNXyQHe3NwcBw4ciGXLlkVXV1dMnz492tvbBz+Ybe/evVFe/scL69/61reir68v/vZv/3bI/bS2tsZXvvKV09s9AAAAnCdK/h7ws8H3gAMAAJDprH8POAAAADAyAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQjCvBVq1bF1KlTo6qqKurr62Pz5s0nnf/9738/rrnmmqiqqorrrrsuNm7cOKLNAgAAwPmq5ABft25dtLS0RGtra2zdujWmTZsWTU1NsX///mHnv/baazFv3ry48847Y9u2bTF37tyYO3duvPHGG6e9eQAAADhflBVFUZSyoL6+Pm688cZ4+umnIyJiYGAg6urq4t57740lS5YcN7+5uTmOHDkSP/rRjwbHPvGJT8T06dNj9erVp/SYvb29UVNTEz09PVFdXV3KdgEAAKBko9GhY0uZ3NfXF1u2bImlS5cOjpWXl0djY2N0dnYOu6azszNaWlqGjDU1NcVLL710wsc5evRoHD16dPDnnp6eiPj9/wAAAAAYbX/ozxKvWZ9USQF+8ODB6O/vj9ra2iHjtbW1sXPnzmHXdHV1DTu/q6vrhI/T1tYWjz322HHjdXV1pWwXAAAATsv//u//Rk1NzRm5r5ICPMvSpUuHXDU/dOhQfPCDH4y9e/eesScO55re3t6oq6uLffv2easF71vOORcC55wLgXPOhaCnpyeuuOKKuPzyy8/YfZYU4OPHj48xY8ZEd3f3kPHu7u6YOHHisGsmTpxY0vyIiMrKyqisrDxuvKamxh9w3veqq6udc973nHMuBM45FwLnnAtBefmZ+/buku6poqIiZsyYER0dHYNjAwMD0dHREQ0NDcOuaWhoGDI/IuKVV1454XwAAAB4Pyr5JegtLS2xcOHCmDlzZsyaNStWrlwZR44ciUWLFkVExIIFC2LKlCnR1tYWERH33Xdf3HLLLfHkk0/G7bffHmvXro2f/exn8eyzz57ZZwIAAADnsJIDvLm5OQ4cOBDLli2Lrq6umD59erS3tw9+0NrevXuHXKK/6aab4oUXXohHHnkkHnroofirv/qreOmll+Laa6895cesrKyM1tbWYV+WDu8XzjkXAuecC4FzzoXAOedCMBrnvOTvAQcAAABKd+beTQ4AAACckAAHAACABAIcAAAAEghwAAAASHDOBPiqVati6tSpUVVVFfX19bF58+aTzv/+978f11xzTVRVVcV1110XGzduTNopjFwp53zNmjUxe/bsGDduXIwbNy4aGxvf888FnAtK/X3+B2vXro2ysrKYO3fu6G4QzoBSz/mhQ4di8eLFMWnSpKisrIyrr77a310455V6zleuXBkf/vCH4+KLL466urp44IEH4ne/+13SbqE0P/nJT2LOnDkxefLkKCsri5deeuk912zatCk+/vGPR2VlZXzoQx+K559/vuTHPScCfN26ddHS0hKtra2xdevWmDZtWjQ1NcX+/fuHnf/aa6/FvHnz4s4774xt27bF3LlzY+7cufHGG28k7xxOXannfNOmTTFv3rx49dVXo7OzM+rq6uK2226Lt99+O3nncOpKPed/sGfPnvjSl74Us2fPTtopjFyp57yvry8+9alPxZ49e2L9+vWxa9euWLNmTUyZMiV553DqSj3nL7zwQixZsiRaW1tjx44d8dxzz8W6devioYceSt45nJojR47EtGnTYtWqVac0/xe/+EXcfvvtceutt8b27dvj/vvvj7vuuitefvnl0h64OAfMmjWrWLx48eDP/f39xeTJk4u2trZh53/2s58tbr/99iFj9fX1xd///d+P6j7hdJR6zv/UsWPHiksvvbT47ne/O1pbhNM2knN+7Nix4qabbiq+/e1vFwsXLiz+5m/+JmGnMHKlnvNvfetbxZVXXln09fVlbRFOW6nnfPHixcVf//VfDxlraWkpbr755lHdJ5wJEVG8+OKLJ53z5S9/ufjYxz42ZKy5ubloamoq6bHO+hXwvr6+2LJlSzQ2Ng6OlZeXR2NjY3R2dg67prOzc8j8iIimpqYTzoezbSTn/E+988478e6778bll18+WtuE0zLSc/7Vr341JkyYEHfeeWfGNuG0jOSc//CHP4yGhoZYvHhx1NbWxrXXXhvLly+P/v7+rG1DSUZyzm+66abYsmXL4MvUd+/eHRs3boxPf/rTKXuG0XamGnTsmdzUSBw8eDD6+/ujtrZ2yHhtbW3s3Llz2DVdXV3Dzu/q6hq1fcLpGMk5/1MPPvhgTJ48+bg/+HCuGMk5/+lPfxrPPfdcbN++PWGHcPpGcs53794d//mf/xmf+9znYuPGjfHWW2/FF7/4xXj33XejtbU1Y9tQkpGc8zvuuCMOHjwYn/zkJ6Moijh27Fjcc889XoLO+8aJGrS3tzd++9vfxsUXX3xK93PWr4AD723FihWxdu3aePHFF6OqqupsbwfOiMOHD8f8+fNjzZo1MX78+LO9HRg1AwMDMWHChHj22WdjxowZ0dzcHA8//HCsXr36bG8NzphNmzbF8uXL45lnnomtW7fGD37wg9iwYUM8/vjjZ3trcE4561fAx48fH2PGjInu7u4h493d3TFx4sRh10ycOLGk+XC2jeSc/8ETTzwRK1asiB//+Mdx/fXXj+Y24bSUes5//vOfx549e2LOnDmDYwMDAxERMXbs2Ni1a1dcddVVo7tpKNFIfp9PmjQpLrroohgzZszg2Ec+8pHo6uqKvr6+qKioGNU9Q6lGcs4fffTRmD9/ftx1110REXHdddfFkSNH4u67746HH344ystd9+P8dqIGra6uPuWr3xHnwBXwioqKmDFjRnR0dAyODQwMREdHRzQ0NAy7pqGhYcj8iIhXXnnlhPPhbBvJOY+I+MY3vhGPP/54tLe3x8yZMzO2CiNW6jm/5ppr4vXXX4/t27cP3j7zmc8MfrpoXV1d5vbhlIzk9/nNN98cb7311uA/MEVEvPnmmzFp0iTxzTlpJOf8nXfeOS6y//CPTr//jCs4v52xBi3t8+FGx9q1a4vKysri+eefL/7nf/6nuPvuu4vLLrus6OrqKoqiKObPn18sWbJkcP5//dd/FWPHji2eeOKJYseOHUVra2tx0UUXFa+//vrZegrwnko95ytWrCgqKiqK9evXF7/61a8Gb4cPHz5bTwHeU6nn/E/5FHTOB6We87179xaXXnpp8Q//8A/Frl27ih/96EfFhAkTiq997Wtn6ynAeyr1nLe2thaXXnpp8W//9m/F7t27i//4j/8orrrqquKzn/3s2XoKcFKHDx8utm3bVmzbtq2IiOKpp54qtm3bVvzyl78siqIolixZUsyfP39w/u7du4tLLrmk+Md//Mdix44dxapVq4oxY8YU7e3tJT3uORHgRVEU//zP/1xcccUVRUVFRTFr1qziv//7vwf/2y233FIsXLhwyPzvfe97xdVXX11UVFQUH/vYx4oNGzYk7xhKV8o5/+AHP1hExHG31tbW/I1DCUr9ff7/E+CcL0o956+99lpRX19fVFZWFldeeWXx9a9/vTh27FjyrqE0pZzzd999t/jKV75SXHXVVUVVVVVRV1dXfPGLXyz+7//+L3/jcApeffXVYf+u/YdzvXDhwuKWW245bs306dOLioqK4sorryz+5V/+peTHLSsKrwkBAACA0XbW3wMOAAAAFwIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkOD/Ac7nRNdHzOW8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ===== COMBINED VISUALIZATION =====\n",
    "# Compare key metrics across all three datasets\n",
    "def create_combined_comparison():\n",
    "    metrics = ['execution_time', 'memory_usage', 'coverage', 'diversity']\n",
    "    datasets = ['whole_dataset', 'first_third', 'last_two_thirds']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        # Set up figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Prepare data\n",
    "        apriori_values = [\n",
    "            evaluator_whole.metrics['apriori'][metric],\n",
    "            evaluator_first_third.metrics['apriori'][metric],\n",
    "            evaluator_last_two_thirds.metrics['apriori'][metric]\n",
    "        ]\n",
    "        \n",
    "        word2vec_values = [\n",
    "            evaluator_whole.metrics['word2vec'][metric],\n",
    "            evaluator_first_third.metrics['word2vec'][metric],\n",
    "            evaluator_last_two_thirds.metrics['word2vec'][metric]\n",
    "        ]\n",
    "        \n",
    "        # Set bar positions\n",
    "        bar_width = 0.35\n",
    "        index = np.arange(len(datasets))\n",
    "        \n",
    "        # Create bars\n",
    "        bar1 = ax.bar(index, apriori_values, bar_width, label='Apriori')\n",
    "        bar2 = ax.bar(index + bar_width, word2vec_values, bar_width, label='Word2Vec')\n",
    "        \n",
    "        # Add labels and title\n",
    "        ax.set_xlabel('Dataset')\n",
    "        ax.set_ylabel(f'{metric.replace(\"_\", \" \").title()}')\n",
    "        ax.set_title(f'Comparison of {metric.replace(\"_\", \" \").title()} Across Datasets')\n",
    "        ax.set_xticks(index + bar_width / 2)\n",
    "        ax.set_xticklabels([d.replace(\"_\", \" \").title() for d in datasets])\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add values on top of bars\n",
    "        for i, v in enumerate(apriori_values):\n",
    "            ax.text(i - 0.05, v + (max(apriori_values + word2vec_values) * 0.02), \n",
    "                    f'{v:.2f}', ha='center', fontsize=9)\n",
    "                    \n",
    "        for i, v in enumerate(word2vec_values):\n",
    "            ax.text(i + bar_width - 0.05, v + (max(apriori_values + word2vec_values) * 0.02), \n",
    "                    f'{v:.2f}', ha='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(path_results, f'combined_{metric}_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    # Also create a table with all metrics\n",
    "    combined_metrics = pd.DataFrame({\n",
    "        'Metric': ['Execution Time (s)', 'Memory Usage (MB)', 'Coverage', 'Diversity'],\n",
    "        'Whole_Apriori': [evaluator_whole.metrics['apriori']['execution_time'], \n",
    "                        evaluator_whole.metrics['apriori']['memory_usage'], \n",
    "                        evaluator_whole.metrics['apriori']['coverage'], \n",
    "                        evaluator_whole.metrics['apriori']['diversity']],\n",
    "        'Whole_Word2Vec': [evaluator_whole.metrics['word2vec']['execution_time'], \n",
    "                            evaluator_whole.metrics['word2vec']['memory_usage'], \n",
    "                            evaluator_whole.metrics['word2vec']['coverage'], \n",
    "                            evaluator_whole.metrics['word2vec']['diversity']],\n",
    "                            'First_Third_Apriori': [evaluator_first_third.metrics['apriori']['execution_time'], \n",
    "                                evaluator_first_third.metrics['apriori']['memory_usage'], \n",
    "                                evaluator_first_third.metrics['apriori']['coverage'], \n",
    "                                evaluator_first_third.metrics['apriori']['diversity']],\n",
    "        'First_Third_Word2Vec': [evaluator_first_third.metrics['word2vec']['execution_time'], \n",
    "                                evaluator_first_third.metrics['word2vec']['memory_usage'], \n",
    "                                evaluator_first_third.metrics['word2vec']['coverage'], \n",
    "                                evaluator_first_third.metrics['word2vec']['diversity']],\n",
    "        'Last_Two_Thirds_Apriori': [evaluator_last_two_thirds.metrics['apriori']['execution_time'], \n",
    "                                    evaluator_last_two_thirds.metrics['apriori']['memory_usage'], \n",
    "                                    evaluator_last_two_thirds.metrics['apriori']['coverage'], \n",
    "                                    evaluator_last_two_thirds.metrics['apriori']['diversity']],\n",
    "        'Last_Two_Thirds_Word2Vec': [evaluator_last_two_thirds.metrics['word2vec']['execution_time'], \n",
    "                                    evaluator_last_two_thirds.metrics['word2vec']['memory_usage'], \n",
    "                                    evaluator_last_two_thirds.metrics['word2vec']['coverage'], \n",
    "                                    evaluator_last_two_thirds.metrics['word2vec']['diversity']]\n",
    "    })\n",
    "    \n",
    "    # Save combined metrics\n",
    "    combined_metrics.to_csv(os.path.join(path_results, 'combined_metrics_comparison.csv'), index=False)\n",
    "    \n",
    "    # Create a readable HTML table for better visualization\n",
    "    html_table = combined_metrics.to_html(index=False)\n",
    "    with open(os.path.join(path_results, 'combined_metrics_comparison.html'), 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "            <style>\n",
    "                table { border-collapse: collapse; width: 100%; }\n",
    "                th, td { text-align: center; padding: 8px; border: 1px solid #ddd; }\n",
    "                th { background-color: #f2f2f2; }\n",
    "                tr:nth-child(even) { background-color: #f9f9f9; }\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "        <h2>Combined Metrics Comparison</h2>\n",
    "        \"\"\")\n",
    "        f.write(html_table)\n",
    "        f.write(\"</body></html>\")\n",
    "    \n",
    "    print(\"Combined comparison visualizations and metrics saved.\")\n",
    "\n",
    "# Import numpy for the bar positions\n",
    "import numpy as np\n",
    "\n",
    "# Create combined comparison\n",
    "create_combined_comparison()\n",
    "\n",
    "print(\"\\nAll evaluations completed and results saved.\")\n",
    "\n",
    "\n",
    "\n",
    "def create_combined_category_comparison():\n",
    "    \"\"\"\n",
    "    Create visualizations and tables comparing cross-category relationships across all data partitions.\n",
    "    \"\"\"\n",
    "    datasets = ['whole_dataset', 'first_third', 'last_two_thirds']\n",
    "    models = ['apriori', 'word2vec']\n",
    "    \n",
    "    # Collect category analysis results from all evaluators\n",
    "    whole_analysis = evaluator_whole.analyze_category_relationships()\n",
    "    first_third_analysis = evaluator_first_third.analyze_category_relationships()\n",
    "    last_two_thirds_analysis = evaluator_last_two_thirds.analyze_category_relationships()\n",
    "    \n",
    "    # Create side-by-side comparison of same vs cross-category percentages\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        ax = axes[i]\n",
    "        \n",
    "  \n",
    "        same_category_values = [\n",
    "            whole_analysis[model]['same_category_percentage'],\n",
    "            first_third_analysis[model]['same_category_percentage'],\n",
    "            last_two_thirds_analysis[model]['same_category_percentage']\n",
    "        ]\n",
    "        \n",
    "        cross_category_values = [\n",
    "            whole_analysis[model]['cross_category_percentage'],\n",
    "            first_third_analysis[model]['cross_category_percentage'],\n",
    "            last_two_thirds_analysis[model]['cross_category_percentage']\n",
    "        ]\n",
    "        \n",
    "       \n",
    "        bar_width = 0.35\n",
    "        index = np.arange(len(datasets))\n",
    "        \n",
    "      \n",
    "        bar1 = ax.bar(index, same_category_values, bar_width, label='Same Category')\n",
    "        bar2 = ax.bar(index + bar_width, cross_category_values, bar_width, label='Cross Category')\n",
    "        \n",
    "       \n",
    "        ax.set_xlabel('Dataset')\n",
    "        ax.set_ylabel('Percentage (%)')\n",
    "        ax.set_title(f'{model.capitalize()} Model: Category Relationships Across Datasets')\n",
    "        ax.set_xticks(index + bar_width / 2)\n",
    "        ax.set_xticklabels([d.replace(\"_\", \" \").title() for d in datasets])\n",
    "        ax.legend()\n",
    "        \n",
    "     \n",
    "        for j, v in enumerate(same_category_values):\n",
    "            ax.text(j - 0.05, v + 2, f'{v:.1f}%', ha='center', fontsize=9)\n",
    "                    \n",
    "        for j, v in enumerate(cross_category_values):\n",
    "            ax.text(j + bar_width - 0.05, v + 2, f'{v:.1f}%', ha='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(path_results, 'combined_category_relationships.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "  \n",
    "    combined_category_metrics = pd.DataFrame({\n",
    "        'Metric': ['Same Category %', 'Cross Category %'],\n",
    "        'Whole_Apriori': [\n",
    "            whole_analysis['apriori']['same_category_percentage'],\n",
    "            whole_analysis['apriori']['cross_category_percentage']\n",
    "        ],\n",
    "        'Whole_Word2Vec': [\n",
    "            whole_analysis['word2vec']['same_category_percentage'],\n",
    "            whole_analysis['word2vec']['cross_category_percentage']\n",
    "        ],\n",
    "        'First_Third_Apriori': [\n",
    "            first_third_analysis['apriori']['same_category_percentage'],\n",
    "            first_third_analysis['apriori']['cross_category_percentage']\n",
    "        ],\n",
    "        'First_Third_Word2Vec': [\n",
    "            first_third_analysis['word2vec']['same_category_percentage'],\n",
    "            first_third_analysis['word2vec']['cross_category_percentage']\n",
    "        ],\n",
    "        'Last_Two_Thirds_Apriori': [\n",
    "            last_two_thirds_analysis['apriori']['same_category_percentage'],\n",
    "            last_two_thirds_analysis['apriori']['cross_category_percentage']\n",
    "        ],\n",
    "        'Last_Two_Thirds_Word2Vec': [\n",
    "            last_two_thirds_analysis['word2vec']['same_category_percentage'],\n",
    "            last_two_thirds_analysis['word2vec']['cross_category_percentage']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "   \n",
    "    combined_category_metrics.to_csv(os.path.join(path_results, 'combined_category_metrics.csv'), index=False)\n",
    "    \n",
    "  \n",
    "    html_table = combined_category_metrics.to_html(index=False)\n",
    "    with open(os.path.join(path_results, 'combined_category_metrics.html'), 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "            <style>\n",
    "                table { border-collapse: collapse; width: 100%; }\n",
    "                th, td { text-align: center; padding: 8px; border: 1px solid #ddd; }\n",
    "                th { background-color: #f2f2f2; }\n",
    "                tr:nth-child(even) { background-color: #f9f9f9; }\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "        <h2>Combined Category Relationships Comparison</h2>\n",
    "        \"\"\")\n",
    "        f.write(html_table)\n",
    "        f.write(\"</body></html>\")\n",
    "    \n",
    "    \n",
    "    compare_top_category_pairs(whole_analysis, first_third_analysis, last_two_thirds_analysis)\n",
    "    \n",
    "    print(\"Combined category relationship comparisons and metrics saved.\")\n",
    "\n",
    "def compare_top_category_pairs(whole_analysis, first_third_analysis, last_two_thirds_analysis):\n",
    "    \"\"\"\n",
    "    Compare top category pairs across different datasets.\n",
    "    \"\"\"\n",
    "    models = ['apriori', 'word2vec']\n",
    "    datasets = ['Whole Dataset', 'First Third', 'Last Two Thirds']\n",
    "    analyses = [whole_analysis, first_third_analysis, last_two_thirds_analysis]\n",
    "    \n",
    "    for model in models:\n",
    "    \n",
    "        plt.figure(figsize=(15, 12))\n",
    "    \n",
    "        all_top_pairs = []\n",
    "        \n",
    "        for i, analysis in enumerate(analyses):\n",
    "            if 'top_category_combinations' in analysis[model]:\n",
    "                top_pairs = analysis[model]['top_category_combinations'].head(5)\n",
    "                \n",
    "       \n",
    "                for _, row in top_pairs.iterrows():\n",
    "                    pair_name = f\"{row['Original Product category']}  {row['Complementary Product category']}\"\n",
    "                    all_top_pairs.append((pair_name, row['frequency'], datasets[i]))\n",
    "        \n",
    "     \n",
    "        unique_pairs = list(set([pair[0] for pair in all_top_pairs]))\n",
    " \n",
    "        pair_frequencies = {dataset: {pair: 0 for pair in unique_pairs} for dataset in datasets}\n",
    "        \n",
    "\n",
    "        for pair_name, freq, dataset in all_top_pairs:\n",
    "            pair_frequencies[dataset][pair_name] = freq\n",
    "        \n",
    "   \n",
    "        df_pairs = pd.DataFrame(pair_frequencies)\n",
    "        \n",
    "    \n",
    "        df_pairs['Total'] = df_pairs.sum(axis=1)\n",
    "        df_pairs = df_pairs.sort_values('Total', ascending=False).head(10)\n",
    "        df_pairs = df_pairs.drop('Total', axis=1)\n",
    "        \n",
    "      \n",
    "        ax = df_pairs.plot(kind='bar', figsize=(15, 8))\n",
    "        \n",
    "        plt.title(f'Top Category Pairs Comparison - {model.capitalize()} Model', fontsize=16)\n",
    "        plt.xlabel('Category Pair', fontsize=14)\n",
    "        plt.ylabel('Frequency', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "     \n",
    "        plt.savefig(os.path.join(path_results, f'{model}_top_category_pairs_comparison.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "       \n",
    "        df_pairs.to_csv(os.path.join(path_results, f'{model}_top_category_pairs_comparison.csv'))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "create_combined_category_comparison()\n",
    "\n",
    "print(\"\\nAll category relationship comparisons completed and results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
