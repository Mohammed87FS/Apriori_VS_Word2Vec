{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "\n",
    "- **How many rows**: Number of roows in the dataset.\n",
    "- **How many columns**: Number of columns in the dataset.\n",
    "- **List of the columns**: List of columns in the dataset.\n",
    "- **Total Products**: Number of products in the dataset.\n",
    "- **Distinct Products**: Number of unique products.\n",
    "- **Total Transactions**: Number of transactions recorded.\n",
    "- **Total Customers**: Number of customers in the dataset.\n",
    "- **Distinct Categories**: Number of unique product categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DIAGNOSTIC ANALYSIS ===\n",
      "df1 shape: (520609, 8)\n",
      "df2 shape: (4185, 2)\n",
      "\n",
      "df1 columns: ['BillNo', 'Itemname', 'Quantity', 'Date', 'Price', 'CustomerID', 'Country', 'category']\n",
      "df2 columns: ['Itemname', 'Corrected_Category']\n",
      "\n",
      "Null values in df1['Itemname']: 0\n",
      "Null values in df2['Itemname']: 0\n",
      "\n",
      "Unique items in df1: 4185\n",
      "Unique items in df2: 4185\n",
      "\n",
      "Items in df1 but NOT in df2: 3\n",
      "Items in df2 but NOT in df1: 3\n",
      "\n",
      "================================================================================\n",
      "ALL ITEMS IN DF1 BUT NOT IN DF2 (3 items):\n",
      "================================================================================\n",
      " 1. 'OOPS ! adjustment'\n",
      " 2. 'Wrongly mrked had 85123a in box'\n",
      " 3. 'crushed ctn'\n",
      "\n",
      "================================================================================\n",
      "ALL ITEMS IN DF2 BUT NOT IN DF1 (3 items):\n",
      "================================================================================\n",
      " 1. 'OOPS ! Adjustment'\n",
      " 2. 'crushed ctn HAPPY STENCIL CRAFT'\n",
      " 3. 'wrongly mrked had 85123a in box'\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS OF UNMATCHED ITEMS FROM DF1:\n",
      "================================================================================\n",
      "Total rows affected by unmatched items: 3\n",
      "\n",
      "Top 10 most frequent unmatched items in df1:\n",
      "  'crushed ctn': 1 occurrences\n",
      "  'OOPS ! adjustment': 1 occurrences\n",
      "  'Wrongly mrked had 85123a in box': 1 occurrences\n",
      "\n",
      "=== STRING ANALYSIS ===\n",
      "Sample items from df1:\n",
      "  'WHITE HANGING HEART T-LIGHT HOLDER' (length: 34)\n",
      "  'WHITE METAL LANTERN' (length: 19)\n",
      "  'CREAM CUPID HEARTS COAT HANGER' (length: 30)\n",
      "  'KNITTED UNION FLAG HOT WATER BOTTLE' (length: 35)\n",
      "  'RED WOOLLY HOTTIE WHITE HEART.' (length: 30)\n",
      "\n",
      "Sample items from df2:\n",
      "  'APPLE BATH SPONGE' (length: 17)\n",
      "  'BATHROOM SCALES FOOTPRINTS IN SAND' (length: 34)\n",
      "  'BATHROOM SCALES RUBBER DUCKS' (length: 28)\n",
      "  'BATHROOM SCALES, TROPICAL BEACH' (length: 31)\n",
      "  'BATHROOM SET LOVE HEART DESIGN' (length: 30)\n",
      "\n",
      "=== MERGE RESULTS ===\n",
      "Original df1 rows: 520609\n",
      "Merged rows: 520609\n",
      "Rows with NO data from df2: 3\n",
      "Rows with data from df2: 520606\n",
      "\n",
      "First 10 items that didn't get matched:\n",
      "  'crushed ctn'\n",
      "  'OOPS ! adjustment'\n",
      "  'Wrongly mrked had 85123a in box'\n",
      "\n",
      "=== CATEGORY VALIDATION CHECK ===\n",
      "Rows with missing categories: 3\n",
      "Filled 3 missing categories with 'Miscellaneous'\n",
      "\n",
      "Merged dataset saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\full_validated_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd() \n",
    "dataset_dir = os.path.join(current_dir, \"Dataset\")\n",
    "\n",
    "excel_file = 'not_validated_dataset_with_category.xlsx'\n",
    "excel_file_path = os.path.join(dataset_dir, excel_file)\n",
    "\n",
    "excel_file2 = 'validated_distinct_products_with_categories.xlsx'\n",
    "excel_file_path2 = os.path.join(dataset_dir, excel_file2)\n",
    "\n",
    "df1 = pd.read_excel(excel_file_path)\n",
    "df2 = pd.read_excel(excel_file_path2)\n",
    "\n",
    "print(\"=== DIAGNOSTIC ANALYSIS ===\")\n",
    "print(f\"df1 shape: {df1.shape}\")\n",
    "print(f\"df2 shape: {df2.shape}\")\n",
    "\n",
    "print(f\"\\ndf1 columns: {list(df1.columns)}\")\n",
    "print(f\"df2 columns: {list(df2.columns)}\")\n",
    "\n",
    "if 'Itemname' not in df1.columns:\n",
    "    print(\" ERROR: 'Itemname' column not found in df1!\")\n",
    "if 'Itemname' not in df2.columns:\n",
    "    print(\" ERROR: 'Itemname' column not found in df2!\")\n",
    "\n",
    "print(f\"\\nNull values in df1['Itemname']: {df1['Itemname'].isnull().sum()}\")\n",
    "print(f\"Null values in df2['Itemname']: {df2['Itemname'].isnull().sum()}\")\n",
    "\n",
    "df1_items = set(df1['Itemname'].dropna())\n",
    "df2_items = set(df2['Itemname'].dropna())\n",
    "\n",
    "print(f\"\\nUnique items in df1: {len(df1_items)}\")\n",
    "print(f\"Unique items in df2: {len(df2_items)}\")\n",
    "\n",
    "items_in_df1_not_in_df2 = df1_items - df2_items\n",
    "items_in_df2_not_in_df1 = df2_items - df1_items\n",
    "\n",
    "print(f\"\\nItems in df1 but NOT in df2: {len(items_in_df1_not_in_df2)}\")\n",
    "print(f\"Items in df2 but NOT in df1: {len(items_in_df2_not_in_df1)}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL ITEMS IN DF1 BUT NOT IN DF2 ({len(items_in_df1_not_in_df2)} items):\")\n",
    "print(f\"{'='*80}\")\n",
    "for i, item in enumerate(sorted(items_in_df1_not_in_df2), 1):\n",
    "    print(f\"{i:2d}. '{item}'\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL ITEMS IN DF2 BUT NOT IN DF1 ({len(items_in_df2_not_in_df1)} items):\")\n",
    "print(f\"{'='*80}\")\n",
    "for i, item in enumerate(sorted(items_in_df2_not_in_df1), 1):\n",
    "    print(f\"{i:2d}. '{item}'\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ANALYSIS OF UNMATCHED ITEMS FROM DF1:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "df1_unmatched_counts = df1[df1['Itemname'].isin(items_in_df1_not_in_df2)]['Itemname'].value_counts()\n",
    "print(f\"Total rows affected by unmatched items: {df1_unmatched_counts.sum()}\")\n",
    "print(\"\\nTop 10 most frequent unmatched items in df1:\")\n",
    "for item, count in df1_unmatched_counts.head(10).items():\n",
    "    print(f\"  '{item}': {count} occurrences\")\n",
    "\n",
    "unmatched_df1 = pd.DataFrame({\n",
    "    'Itemname': list(items_in_df1_not_in_df2),\n",
    "    'Frequency_in_df1': [df1['Itemname'].value_counts().get(item, 0) for item in items_in_df1_not_in_df2]\n",
    "}).sort_values('Frequency_in_df1', ascending=False)\n",
    "\n",
    "unmatched_df2 = pd.DataFrame({\n",
    "    'Itemname': list(items_in_df2_not_in_df1)\n",
    "})\n",
    "\n",
    "print(f\"\\n=== STRING ANALYSIS ===\")\n",
    "print(\"Sample items from df1:\")\n",
    "for item in df1['Itemname'].dropna().head(5):\n",
    "    print(f\"  '{item}' (length: {len(str(item))})\")\n",
    "\n",
    "print(\"\\nSample items from df2:\")\n",
    "for item in df2['Itemname'].dropna().head(5):\n",
    "    print(f\"  '{item}' (length: {len(str(item))})\")\n",
    "\n",
    "merged_df = df1.merge(df2, on='Itemname', how='left')\n",
    "\n",
    "print(f\"\\n=== MERGE RESULTS ===\")\n",
    "print(f\"Original df1 rows: {len(df1)}\")\n",
    "print(f\"Merged rows: {len(merged_df)}\")\n",
    "\n",
    "df2_columns = [col for col in df2.columns if col != 'Itemname']\n",
    "if df2_columns:\n",
    "    missing_data_mask = merged_df[df2_columns].isnull().all(axis=1)\n",
    "    rows_with_missing_data = missing_data_mask.sum()\n",
    "    print(f\"Rows with NO data from df2: {rows_with_missing_data}\")\n",
    "    print(f\"Rows with data from df2: {len(merged_df) - rows_with_missing_data}\")\n",
    "    \n",
    "    if rows_with_missing_data > 0:\n",
    "        print(f\"\\nFirst 10 items that didn't get matched:\")\n",
    "        unmatched_items = merged_df[missing_data_mask]['Itemname'].unique()[:10]\n",
    "        for item in unmatched_items:\n",
    "            print(f\"  '{item}'\")\n",
    "\n",
    "output_file = 'full_validated_dataset.xlsx'\n",
    "output_file_path = os.path.join(dataset_dir, output_file)\n",
    "\n",
    "if 'category' in merged_df.columns:\n",
    "    merged_df = merged_df.drop('category', axis=1)\n",
    "\n",
    "if 'Corrected_Category' in merged_df.columns:\n",
    "    merged_df = merged_df.rename(columns={'Corrected_Category': 'category'})\n",
    "\n",
    "# ===== CATEGORY VALIDATION CHECK =====\n",
    "print(f\"\\n=== CATEGORY VALIDATION CHECK ===\")\n",
    "\n",
    "\n",
    "if 'category' in merged_df.columns:\n",
    "  \n",
    "    missing_categories = merged_df['category'].isnull().sum()\n",
    "    print(f\"Rows with missing categories: {missing_categories}\")\n",
    "    \n",
    "    if missing_categories > 0:\n",
    "      \n",
    "        merged_df['category'] = merged_df['category'].fillna('Miscellaneous')\n",
    "        print(f\"Filled {missing_categories} missing categories with 'Miscellaneous'\")\n",
    "    else:\n",
    "        print(\"No missing categories found - all rows have valid categories\")\n",
    "    \n",
    "   \n",
    "    category_counts = merged_df['category'].value_counts()\n",
    "  \n",
    "else:\n",
    "    print(\"Warning: No 'category' column found in merged dataset!\")\n",
    "\n",
    "merged_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"\\nMerged dataset saved to: {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing Miscellaneous category: 517587 records\n",
      "\n",
      " ===== DATA ANALYSIS ===== \n",
      "\n",
      "Dataset Overview (Whole Dataset):\n",
      "==================================================\n",
      "  Rows: 517,587\n",
      "  Columns: 8\n",
      "  Column Names: BillNo, Itemname, Quantity, Date, Price, CustomerID, Country, category\n",
      "==================================================\n",
      "  Total Products: 517,587\n",
      "  Distinct Products: 4,009\n",
      "  Total Transactions: 19,505\n",
      "  Total Customers: 4,297\n",
      "  Distinct Categories: 19\n",
      "==================================================\n",
      "\n",
      "Comparison Table:\n",
      "                     Full Dataset  First Third  Last Two Thirds\n",
      "Metric                                                         \n",
      "Rows                       517587       172529           345058\n",
      "Columns                         8            8                8\n",
      "Total Products             517587       172529           345058\n",
      "Distinct Products            4009         3254             3615\n",
      "Total Transactions          19505         6706            12800\n",
      "Total Customers              4297         2487             3632\n",
      "Distinct Categories            19           19               19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "BACKGROUND_COLOR = \"#f5f5f5\" \n",
    "TEXT_COLOR = \"#333333\"       \n",
    "\n",
    "\n",
    "current_dir = os.getcwd() \n",
    "dataset_dir = os.path.join(current_dir, \"Dataset\")\n",
    "\n",
    "\n",
    "excel_file = 'full_validated_dataset.xlsx'\n",
    "excel_file_path = os.path.join(dataset_dir, excel_file)\n",
    "\n",
    "# ===== DATA LOADING =====\n",
    "\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "\n",
    "df = df[df['category'] != 'Miscellaneous']\n",
    "print(f\"Dataset after removing Miscellaneous category: {len(df)} records\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ===== DATA ANALYSIS ===== \\n\")\n",
    "\n",
    "\n",
    "plots_dir = os.path.join(current_dir, 'Analysis_plots')\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "# ===== FUNCTION DEFINITION =====\n",
    "def compute_stats(dataframe):\n",
    "    \"\"\"\n",
    "    Compute key statistics for the given DataFrame.\n",
    "    Returns a dictionary with analysis results.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    stats['Rows'] = len(dataframe)\n",
    "    stats['Columns'] = len(dataframe.columns)\n",
    "    stats['Total Products'] = len(dataframe)  \n",
    "    stats['Distinct Products'] = dataframe['Itemname'].nunique()\n",
    "    stats['Total Transactions'] = dataframe['BillNo'].nunique()\n",
    "    stats['Total Customers'] = int(dataframe['CustomerID'].dropna().nunique())\n",
    "    stats['Distinct Categories'] = dataframe['category'].nunique()\n",
    "    return stats\n",
    "\n",
    "# ===== ANALYSIS FOR DIFFERENT PARTITIONS =====\n",
    "\n",
    "stats_whole = compute_stats(df)\n",
    "\n",
    "\n",
    "one_third_index = len(df) // 3\n",
    "\n",
    "df_first_third = df.iloc[:one_third_index]\n",
    "df_last_two_thirds = df.iloc[one_third_index:]\n",
    "\n",
    "stats_first_third = compute_stats(df_first_third)\n",
    "stats_last_two_thirds = compute_stats(df_last_two_thirds)\n",
    "\n",
    "# ===== PRINT OVERVIEW FOR WHOLE DATASET =====\n",
    "print(\"Dataset Overview (Whole Dataset):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Rows: {stats_whole['Rows']:,}\")\n",
    "print(f\"  Columns: {stats_whole['Columns']}\")\n",
    "print(f\"  Column Names: {', '.join(df.columns.tolist())}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Total Products: {stats_whole['Total Products']:,}\")\n",
    "print(f\"  Distinct Products: {stats_whole['Distinct Products']:,}\")\n",
    "print(f\"  Total Transactions: {stats_whole['Total Transactions']:,}\")\n",
    "print(f\"  Total Customers: {stats_whole['Total Customers']:,}\")\n",
    "print(f\"  Distinct Categories: {stats_whole['Distinct Categories']}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ===== COMPARISON TABLE =====\n",
    "\n",
    "comparison_data = {\n",
    "    \"Full Dataset\": stats_whole,\n",
    "    \"First Third\": stats_first_third,\n",
    "    \"Last Two Thirds\": stats_last_two_thirds\n",
    "}\n",
    "comparison_table = pd.DataFrame(comparison_data)\n",
    "comparison_table.index.name = \"Metric\"\n",
    "\n",
    "print(\"\\nComparison Table:\")\n",
    "print(comparison_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing Miscellaneous category: 517587 records\n",
      "Excel file saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\distinct_products_with_categories.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd() \n",
    "dataset_dir = os.path.join(current_dir, \"Dataset\")\n",
    "\n",
    "\n",
    "excel_file = 'full_validated_dataset.xlsx'\n",
    "excel_file_path = os.path.join(dataset_dir, excel_file)\n",
    "\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "data_excel = load_dataset(excel_file_path)\n",
    "\n",
    "data_excel = data_excel[data_excel['category'] != 'Miscellaneous']\n",
    "print(f\"Dataset after removing Miscellaneous category: {len(data_excel)} records\")\n",
    "\n",
    "\n",
    "\n",
    "data_excel.dropna(subset=['Itemname'], inplace=True)   \n",
    "output_file = os.path.join(current_dir, 'unique_items.xlsx')\n",
    "\n",
    "unique_items_df = pd.DataFrame({'unique_items': data_excel['Itemname'].unique()})\n",
    "unique_items_df.to_excel(output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "distinct_products_df = data_excel[['Itemname', 'category']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "distinct_products_df = distinct_products_df.sort_values(['category', 'Itemname'])\n",
    "\n",
    "\n",
    "output_file = os.path.join(dataset_dir, 'distinct_products_with_categories.xlsx')\n",
    "distinct_products_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Excel file saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Patterns\n",
    "\n",
    "Analyze the purchasing patterns visible in the data:\n",
    "\n",
    "- **Average items per transaction**.\n",
    "- **Distribution of transaction sizes**: Histogram showing the number of items per invoice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing Miscellaneous category: 517587 records\n",
      "\n",
      " ===== TRANSACTION PATTERNS ANALYSIS ===== \n",
      "\n",
      "Average Items per Transaction (Whole Dataset):\n",
      "==================================================\n",
      " Average Items: 26.54\n",
      " Median Items: 15.0\n",
      " Min Items: 1\n",
      " Max Items: 1111\n",
      "==================================================\n",
      "\n",
      "Transaction Size Percentiles (Whole Dataset):\n",
      " 25th percentile: 6.0 items\n",
      " 50th percentile (median): 15.0 items\n",
      " 75th percentile: 29.0 items\n",
      " 90th percentile: 54.0 items\n",
      " 95th percentile: 79.0 items\n",
      " 99th percentile: 222.0 items\n",
      "==================================================\n",
      "\n",
      "Comparison Table (Transaction Metrics):\n",
      "                    Whole  First Third  Last Two Thirds\n",
      "Metric                                                 \n",
      "Avg Items       26.536119    25.727557        26.957656\n",
      "Median Items    15.000000    15.000000        15.000000\n",
      "Min Items        1.000000     1.000000         1.000000\n",
      "Max Items     1111.000000   673.000000      1111.000000\n",
      "P25              6.000000     6.000000         6.000000\n",
      "P50             15.000000    15.000000        15.000000\n",
      "P75             29.000000    28.000000        29.000000\n",
      "P90             54.000000    52.000000        55.000000\n",
      "P95             79.000000    73.000000        81.000000\n",
      "P99            221.960000   216.950000       223.000000\n",
      "\n",
      "Comparison table saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\transaction_comparison_table.xlsx\n",
      "Plot saved for Whole Dataset at: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\transaction_size_distribution_Whole_Dataset.png\n",
      "Plot saved for First Third at: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\transaction_size_distribution_First_Third.png\n",
      "Plot saved for Last Two Thirds at: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\transaction_size_distribution_Last_Two_Thirds.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# ===== VISUALIZATION SETTINGS =====\n",
    "MAIN_COLOR = \"#1f77b4\"       \n",
    "TERTIARY_COLOR = \"#2ca02c\"   \n",
    "BACKGROUND_COLOR = \"#f5f5f5\"\n",
    "TEXT_COLOR = \"#333333\"       \n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette([MAIN_COLOR, TERTIARY_COLOR])\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['axes.facecolor'] = BACKGROUND_COLOR\n",
    "plt.rcParams['axes.edgecolor'] = TEXT_COLOR\n",
    "plt.rcParams['axes.labelcolor'] = TEXT_COLOR\n",
    "plt.rcParams['text.color'] = TEXT_COLOR\n",
    "plt.rcParams['xtick.color'] = TEXT_COLOR\n",
    "plt.rcParams['ytick.color'] = TEXT_COLOR\n",
    "\n",
    "# ===== DATA LOADING =====\n",
    "current_dir = os.getcwd() \n",
    "dataset_dir = os.path.join(current_dir, \"Dataset\")\n",
    "excel_file = 'full_validated_dataset.xlsx'\n",
    "excel_file_path = os.path.join(dataset_dir, excel_file)\n",
    "\n",
    "\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "df = df[df['category'] != 'Miscellaneous']\n",
    "print(f\"Dataset after removing Miscellaneous category: {len(df)} records\")\n",
    "\n",
    "\n",
    "if 'Date' in df.columns:\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "plots_dir = os.path.join(dataset_dir, 'Analysis_plots')\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "# ===== HELPER FUNCTION FOR TRANSACTION ANALYSIS =====\n",
    "def compute_transaction_metrics(dataframe):\n",
    "    \"\"\"\n",
    "    Compute transaction metrics:\n",
    "      - items per transaction: average, median, min, max\n",
    "      - transaction size percentiles (25th, 50th, 75th, 90th, 95th, 99th)\n",
    "    Returns a tuple with (metrics dictionary, items_per_transaction series)\n",
    "    \"\"\"\n",
    "    \n",
    "    items_per_transaction = dataframe.groupby('BillNo').size()\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['Avg Items'] = items_per_transaction.mean()\n",
    "    metrics['Median Items'] = items_per_transaction.median()\n",
    "    metrics['Min Items'] = items_per_transaction.min()\n",
    "    metrics['Max Items'] = items_per_transaction.max()\n",
    "    \n",
    "    \n",
    "    percentiles = np.percentile(items_per_transaction, [25, 50, 75, 90, 95, 99])\n",
    "    metrics['P25'] = percentiles[0]\n",
    "    metrics['P50'] = percentiles[1] \n",
    "    metrics['P75'] = percentiles[2]\n",
    "    metrics['P90'] = percentiles[3]\n",
    "    metrics['P95'] = percentiles[4]\n",
    "    metrics['P99'] = percentiles[5]\n",
    "    \n",
    "    return metrics, items_per_transaction\n",
    "\n",
    "# ===== DATA PARTITIONING =====\n",
    "\n",
    "n = len(df)\n",
    "one_third_index = n // 3\n",
    "\n",
    "df_whole = df.copy()\n",
    "df_first_third = df.iloc[:one_third_index]\n",
    "df_last_two_thirds = df.iloc[one_third_index:]\n",
    "\n",
    "# ===== ANALYZE TRANSACTION PATTERNS =====\n",
    "print(\"\\n ===== TRANSACTION PATTERNS ANALYSIS ===== \\n\")\n",
    "\n",
    "\n",
    "metrics_whole, items_whole = compute_transaction_metrics(df_whole)\n",
    "metrics_first_third, items_first_third = compute_transaction_metrics(df_first_third)\n",
    "metrics_last_two_thirds, items_last_two_thirds = compute_transaction_metrics(df_last_two_thirds)\n",
    "\n",
    "\n",
    "print(\"Average Items per Transaction (Whole Dataset):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\" Average Items: {metrics_whole['Avg Items']:.2f}\")\n",
    "print(f\" Median Items: {metrics_whole['Median Items']}\")\n",
    "print(f\" Min Items: {metrics_whole['Min Items']}\")\n",
    "print(f\" Max Items: {metrics_whole['Max Items']}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTransaction Size Percentiles (Whole Dataset):\")\n",
    "print(f\" 25th percentile: {metrics_whole['P25']:.1f} items\")\n",
    "print(f\" 50th percentile (median): {metrics_whole['P50']:.1f} items\")\n",
    "print(f\" 75th percentile: {metrics_whole['P75']:.1f} items\")\n",
    "print(f\" 90th percentile: {metrics_whole['P90']:.1f} items\")\n",
    "print(f\" 95th percentile: {metrics_whole['P95']:.1f} items\")\n",
    "print(f\" 99th percentile: {metrics_whole['P99']:.1f} items\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ===== COMPARISON TABLE =====\n",
    "comparison_data = {\n",
    "    \"Whole\": metrics_whole,\n",
    "    \"First Third\": metrics_first_third,\n",
    "    \"Last Two Thirds\": metrics_last_two_thirds\n",
    "}\n",
    "comparison_table = pd.DataFrame(comparison_data)\n",
    "comparison_table.index.name = \"Metric\"\n",
    "print(\"\\nComparison Table (Transaction Metrics):\")\n",
    "print(comparison_table)\n",
    "\n",
    "\n",
    "comparison_table_file = os.path.join(plots_dir, \"transaction_comparison_table.xlsx\")\n",
    "comparison_table.to_excel(comparison_table_file)\n",
    "print(f\"\\nComparison table saved to: {comparison_table_file}\")\n",
    "\n",
    "# ===== PLOTTING FUNCTION =====\n",
    "def plot_transaction_distribution(items_series, title, save_path):\n",
    "    \"\"\"\n",
    "    Plot the transaction size distribution given a series of items per transaction.\n",
    "    Saves the plot to the provided save_path.\n",
    "    \"\"\"\n",
    "  \n",
    "    size_bins = [1, 5, 10, 15, 20, 30, 50, 100, np.inf]\n",
    "    size_labels = ['1-4', '5-9', '10-14', '15-19', '20-29', '30-49', '50-99', '100+']\n",
    "    \n",
    " \n",
    "    transaction_binned = pd.cut(items_series, bins=size_bins, labels=size_labels)\n",
    "    size_counts = transaction_binned.value_counts().sort_index()\n",
    "    total_transactions = len(items_series)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(12, 7), facecolor=BACKGROUND_COLOR)\n",
    "    bars = plt.bar(size_counts.index.astype(str), size_counts.values, color=MAIN_COLOR, edgecolor='white')\n",
    "    plt.title(f'Transaction Size Distribution - {title}', fontweight='bold', fontsize=16, color=TEXT_COLOR)\n",
    "    plt.xlabel('Number of Items', fontsize=14, color=TEXT_COLOR)\n",
    "    plt.ylabel('Transactions', fontsize=14, color=TEXT_COLOR)\n",
    "    \n",
    "\n",
    "    for i, v in enumerate(size_counts.values):\n",
    "        percentage = (v / total_transactions) * 100\n",
    "        plt.text(i, v + 0.05 * total_transactions, f\"{percentage:.1f}%\", ha='center', fontsize=10, \n",
    "                 fontweight='bold', color=TEXT_COLOR)\n",
    "    \n",
    "  \n",
    "    max_bin = size_counts.idxmax()\n",
    "    max_idx = list(size_counts.index).index(max_bin)\n",
    "    bars[max_idx].set_edgecolor('black')\n",
    "    bars[max_idx].set_linewidth(2)\n",
    "    \n",
    "    plt.grid(axis='y', alpha=0.3, color=\"#cccccc\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()  \n",
    "\n",
    "# ===== GENERATE AND SAVE SEPARATE PLOTS =====\n",
    "\n",
    "partitions = {\n",
    "    \"Whole_Dataset\": items_whole,\n",
    "    \"First_Third\": items_first_third,\n",
    "    \"Last_Two_Thirds\": items_last_two_thirds\n",
    "}\n",
    "\n",
    "for partition_name, items_series in partitions.items():\n",
    "    plot_title = partition_name.replace(\"_\", \" \")\n",
    "    save_file = os.path.join(plots_dir, f\"transaction_size_distribution_{partition_name}.png\")\n",
    "    plot_transaction_distribution(items_series, plot_title, save_file)\n",
    "    print(f\"Plot saved for {plot_title} at: {save_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products Analysis\n",
    "\n",
    "Examine the product-related characteristics:\n",
    "\n",
    "- **Top 10 most frequently purchased products**.\n",
    "- **Product category distribution**: Percentage of items in each category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PRODUCT ANALYSIS SCRIPT STARTED\n",
      "================================================================================\n",
      "\n",
      "Setting up visualization configuration...\n",
      "Visualization settings configured successfully\n",
      "\n",
      "Loading dataset...\n",
      "Current working directory: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\n",
      "Dataset directory: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\n",
      "Excel file path: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\full_validated_dataset.xlsx\n",
      "Reading Excel file...\n",
      "Dataset loaded successfully with 520609 records\n",
      "Dataset shape: (520609, 8)\n",
      "Columns: ['BillNo', 'Itemname', 'Quantity', 'Date', 'Price', 'CustomerID', 'Country', 'category']\n",
      "\n",
      "Filtering out 'Miscellaneous' category...\n",
      "Original records: 520,609\n",
      "After filtering: 517,587\n",
      "Removed 3,022 records (0.6%)\n",
      "\n",
      "Checking Date column...\n",
      "Date column found\n",
      "Date column already in datetime format\n",
      "\n",
      "Setting up plots directory...\n",
      "Plots directory: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\n",
      "Plots directory already exists\n",
      "\n",
      "Partitioning dataset...\n",
      "Total records: 517,587\n",
      "One-third index: 172,529\n",
      "Whole dataset: 517,587 records\n",
      "First third: 172,529 records\n",
      "Last two thirds: 345,058 records\n",
      "Created 3 data partitions\n",
      "\n",
      "================================================================================\n",
      "GENERATING COMPREHENSIVE ANALYSIS FOR ALL PARTITIONS\n",
      "================================================================================\n",
      "\n",
      "[1/3] PROCESSING PARTITION: WHOLE_DATASET\n",
      "Partition size: 517,587 records\n",
      "Date range: 2010-12-01 08:26:00 to 2011-12-09 12:50:00\n",
      "\n",
      "======================================================================\n",
      "PROCESSING TOP PRODUCTS FOR: WHOLE_DATASET\n",
      "======================================================================\n",
      "Analyzing 517,587 records...\n",
      "\n",
      "OVERALL STATISTICS:\n",
      "  Total items in partition: 517,587\n",
      "  Unique products in partition: 4,009\n",
      "  Coverage of top 10: 16,247 items (3.1%)\n",
      "\n",
      "TOP 10 PRODUCTS BREAKDOWN:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank Product Name                             Count      Percentage  \n",
      "--------------------------------------------------------------------------------\n",
      "1    WHITE HANGING HEART T-LIGHT HOLDER       2269       0.44        %\n",
      "2    JUMBO BAG RED RETROSPOT                  2087       0.40        %\n",
      "3    REGENCY CAKESTAND 3 TIER                 1930       0.37        %\n",
      "4    PARTY BUNTING                            1677       0.32        %\n",
      "5    LUNCH BAG RED RETROSPOT                  1570       0.30        %\n",
      "6    ASSORTED COLOUR BIRD ORNAMENT            1465       0.28        %\n",
      "7    SET OF 3 CAKE TINS PANTRY DESIGN         1360       0.26        %\n",
      "8    PACK OF 72 RETROSPOT CAKE CASES          1328       0.26        %\n",
      "9    LUNCH BAG  BLACK SKULL.                  1315       0.25        %\n",
      "10   NATURAL SLATE HEART CHALKBOARD           1246       0.24        %\n",
      "--------------------------------------------------------------------------------\n",
      "Top product: 'WHITE HANGING HEART T-LIGHT HOLDER' with 2,269 occurrences (0.4%)\n",
      "Least frequent in top 10: 'NATURAL SLATE HEART CHALKBOARD' with 1,246 occurrences (0.2%)\n",
      "Remaining 3,999 products: 501,340 items (avg 125.4 per product)\n",
      "\n",
      "Creating horizontal bar chart...\n",
      "Top products plot saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\top_products_Whole_Dataset.png\n",
      "\n",
      "======================================================================\n",
      "PROCESSING CATEGORY DISTRIBUTION FOR: WHOLE_DATASET\n",
      "======================================================================\n",
      "Analyzing 517,587 records...\n",
      "\n",
      "OVERALL CATEGORY STATISTICS:\n",
      "  Total items in partition: 517,587\n",
      "  Unique categories in partition: 19\n",
      "  Most popular category: 'Kitchen & Dining' with 105,748 items (20.4%)\n",
      "  Least popular category: 'Books & Magazines' with 848 items (0.2%)\n",
      "\n",
      "COMPLETE CATEGORY BREAKDOWN:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank Category Name                  Count        Percentage   Cumulative %\n",
      "--------------------------------------------------------------------------------\n",
      "1    Kitchen & Dining               105,748      20.4        % 20.4        %\n",
      "2    Home Decor                     97,512       18.8        % 39.3        %\n",
      "3    Stationery & Office            46,984       9.1         % 48.3        %\n",
      "4    Seasonal & Holidays            40,666       7.9         % 56.2        %\n",
      "5    Kids & Toys                    40,375       7.8         % 64.0        %\n",
      "6    Textiles & Cozy Items          25,611       4.9         % 69.0        %\n",
      "7    Vintage & Collectibles         24,308       4.7         % 73.7        %\n",
      "8    Fashion & Accessories          24,189       4.7         % 78.3        %\n",
      "9    Party Supplies                 23,609       4.6         % 82.9        %\n",
      "10   Storage & Organization         22,278       4.3         % 87.2        %\n",
      "11   Garden & Outdoor               18,558       3.6         % 90.8        %\n",
      "12   Crafts & DIY                   17,451       3.4         % 94.1        %\n",
      "13   Lighting                       7,022        1.4         % 95.5        %\n",
      "14   Games & Puzzles                7,005        1.4         % 96.9        %\n",
      "15   Electronics & Gadgets          5,227        1.0         % 97.9        %\n",
      "16   Bath & Body                    5,055        1.0         % 98.8        %\n",
      "17   Gifts & Special Occasion       3,129        0.6         % 99.4        %\n",
      "18   Pets & Animals                 2,012        0.4         % 99.8        %\n",
      "19   Books & Magazines              848          0.2         % 100.0       %\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "CATEGORY SIZE ANALYSIS:\n",
      "  Large categories (≥10%): 2 categories, 203,260 items (39.3%)\n",
      "  Medium categories (5-10%): 3 categories, 128,025 items (24.7%)\n",
      "  Small categories (1-5%): 10 categories, 175,258 items (33.9%)\n",
      "  Tiny categories (<1%): 4 categories, 11,044 items (2.1%)\n",
      "\n",
      "Creating bar chart for top 10 categories...\n",
      "Category distribution plot saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\category_distribution_Whole_Dataset.png\n",
      "\n",
      "Completed processing for Whole_Dataset\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[2/3] PROCESSING PARTITION: FIRST_THIRD\n",
      "Partition size: 172,529 records\n",
      "Date range: 2010-12-01 08:26:00 to 2011-05-08 16:13:00\n",
      "\n",
      "======================================================================\n",
      "PROCESSING TOP PRODUCTS FOR: FIRST_THIRD\n",
      "======================================================================\n",
      "Analyzing 172,529 records...\n",
      "\n",
      "OVERALL STATISTICS:\n",
      "  Total items in partition: 172,529\n",
      "  Unique products in partition: 3,254\n",
      "  Coverage of top 10: 6,342 items (3.7%)\n",
      "\n",
      "TOP 10 PRODUCTS BREAKDOWN:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank Product Name                             Count      Percentage  \n",
      "--------------------------------------------------------------------------------\n",
      "1    WHITE HANGING HEART T-LIGHT HOLDER       979        0.57        %\n",
      "2    REGENCY CAKESTAND 3 TIER                 845        0.49        %\n",
      "3    JUMBO BAG RED RETROSPOT                  693        0.40        %\n",
      "4    PARTY BUNTING                            600        0.35        %\n",
      "5    SET OF 3 CAKE TINS PANTRY DESIGN         569        0.33        %\n",
      "6    PACK OF 72 RETROSPOT CAKE CASES          562        0.33        %\n",
      "7    LUNCH BAG RED RETROSPOT                  536        0.31        %\n",
      "8    NATURAL SLATE HEART CHALKBOARD           530        0.31        %\n",
      "9    ASSORTED COLOUR BIRD ORNAMENT            519        0.30        %\n",
      "10   JAM MAKING SET WITH JARS                 509        0.30        %\n",
      "--------------------------------------------------------------------------------\n",
      "Top product: 'WHITE HANGING HEART T-LIGHT HOLDER' with 979 occurrences (0.6%)\n",
      "Least frequent in top 10: 'JAM MAKING SET WITH JARS' with 509 occurrences (0.3%)\n",
      "Remaining 3,244 products: 166,187 items (avg 51.2 per product)\n",
      "\n",
      "Creating horizontal bar chart...\n",
      "Top products plot saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\top_products_First_Third.png\n",
      "\n",
      "======================================================================\n",
      "PROCESSING CATEGORY DISTRIBUTION FOR: FIRST_THIRD\n",
      "======================================================================\n",
      "Analyzing 172,529 records...\n",
      "\n",
      "OVERALL CATEGORY STATISTICS:\n",
      "  Total items in partition: 172,529\n",
      "  Unique categories in partition: 19\n",
      "  Most popular category: 'Kitchen & Dining' with 38,930 items (22.6%)\n",
      "  Least popular category: 'Books & Magazines' with 277 items (0.2%)\n",
      "\n",
      "COMPLETE CATEGORY BREAKDOWN:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank Category Name                  Count        Percentage   Cumulative %\n",
      "--------------------------------------------------------------------------------\n",
      "1    Kitchen & Dining               38,930       22.6        % 22.6        %\n",
      "2    Home Decor                     34,499       20.0        % 42.6        %\n",
      "3    Stationery & Office            14,543       8.4         % 51.0        %\n",
      "4    Kids & Toys                    14,109       8.2         % 59.2        %\n",
      "5    Seasonal & Holidays            9,084        5.3         % 64.4        %\n",
      "6    Party Supplies                 8,563        5.0         % 69.4        %\n",
      "7    Fashion & Accessories          8,514        4.9         % 74.3        %\n",
      "8    Textiles & Cozy Items          7,702        4.5         % 78.8        %\n",
      "9    Vintage & Collectibles         7,468        4.3         % 83.1        %\n",
      "10   Garden & Outdoor               7,194        4.2         % 87.3        %\n",
      "11   Storage & Organization         6,697        3.9         % 91.2        %\n",
      "12   Crafts & DIY                   5,550        3.2         % 94.4        %\n",
      "13   Lighting                       2,305        1.3         % 95.7        %\n",
      "14   Electronics & Gadgets          1,843        1.1         % 96.8        %\n",
      "15   Bath & Body                    1,765        1.0         % 97.8        %\n",
      "16   Games & Puzzles                1,582        0.9         % 98.7        %\n",
      "17   Gifts & Special Occasion       1,230        0.7         % 99.4        %\n",
      "18   Pets & Animals                 674          0.4         % 99.8        %\n",
      "19   Books & Magazines              277          0.2         % 100.0       %\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "CATEGORY SIZE ANALYSIS:\n",
      "  Large categories (≥10%): 2 categories, 73,429 items (42.6%)\n",
      "  Medium categories (5-10%): 3 categories, 37,736 items (21.9%)\n",
      "  Small categories (1-5%): 10 categories, 57,601 items (33.4%)\n",
      "  Tiny categories (<1%): 4 categories, 3,763 items (2.2%)\n",
      "\n",
      "Creating bar chart for top 10 categories...\n",
      "Category distribution plot saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\category_distribution_First_Third.png\n",
      "\n",
      "Completed processing for First_Third\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[3/3] PROCESSING PARTITION: LAST_TWO_THIRDS\n",
      "Partition size: 345,058 records\n",
      "Date range: 2011-05-08 16:13:00 to 2011-12-09 12:50:00\n",
      "\n",
      "======================================================================\n",
      "PROCESSING TOP PRODUCTS FOR: LAST_TWO_THIRDS\n",
      "======================================================================\n",
      "Analyzing 345,058 records...\n",
      "\n",
      "OVERALL STATISTICS:\n",
      "  Total items in partition: 345,058\n",
      "  Unique products in partition: 3,615\n",
      "  Coverage of top 10: 10,878 items (3.2%)\n",
      "\n",
      "TOP 10 PRODUCTS BREAKDOWN:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank Product Name                             Count      Percentage  \n",
      "--------------------------------------------------------------------------------\n",
      "1    JUMBO BAG RED RETROSPOT                  1394       0.40        %\n",
      "2    WHITE HANGING HEART T-LIGHT HOLDER       1290       0.37        %\n",
      "3    SPOTTY BUNTING                           1104       0.32        %\n",
      "4    REGENCY CAKESTAND 3 TIER                 1085       0.31        %\n",
      "5    PARTY BUNTING                            1077       0.31        %\n",
      "6    LUNCH BAG RED RETROSPOT                  1034       0.30        %\n",
      "7    RABBIT NIGHT LIGHT                       1021       0.30        %\n",
      "8    PAPER CHAIN KIT 50'S CHRISTMAS           986        0.29        %\n",
      "9    ASSORTED COLOUR BIRD ORNAMENT            946        0.27        %\n",
      "10   LUNCH BAG APPLE DESIGN                   941        0.27        %\n",
      "--------------------------------------------------------------------------------\n",
      "Top product: 'JUMBO BAG RED RETROSPOT' with 1,394 occurrences (0.4%)\n",
      "Least frequent in top 10: 'LUNCH BAG APPLE DESIGN' with 941 occurrences (0.3%)\n",
      "Remaining 3,605 products: 334,180 items (avg 92.7 per product)\n",
      "\n",
      "Creating horizontal bar chart...\n",
      "Top products plot saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\top_products_Last_Two_Thirds.png\n",
      "\n",
      "======================================================================\n",
      "PROCESSING CATEGORY DISTRIBUTION FOR: LAST_TWO_THIRDS\n",
      "======================================================================\n",
      "Analyzing 345,058 records...\n",
      "\n",
      "OVERALL CATEGORY STATISTICS:\n",
      "  Total items in partition: 345,058\n",
      "  Unique categories in partition: 19\n",
      "  Most popular category: 'Kitchen & Dining' with 66,818 items (19.4%)\n",
      "  Least popular category: 'Books & Magazines' with 571 items (0.2%)\n",
      "\n",
      "COMPLETE CATEGORY BREAKDOWN:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank Category Name                  Count        Percentage   Cumulative %\n",
      "--------------------------------------------------------------------------------\n",
      "1    Kitchen & Dining               66,818       19.4        % 19.4        %\n",
      "2    Home Decor                     63,013       18.3        % 37.6        %\n",
      "3    Stationery & Office            32,441       9.4         % 47.0        %\n",
      "4    Seasonal & Holidays            31,582       9.2         % 56.2        %\n",
      "5    Kids & Toys                    26,266       7.6         % 63.8        %\n",
      "6    Textiles & Cozy Items          17,909       5.2         % 69.0        %\n",
      "7    Vintage & Collectibles         16,840       4.9         % 73.9        %\n",
      "8    Fashion & Accessories          15,675       4.5         % 78.4        %\n",
      "9    Storage & Organization         15,581       4.5         % 82.9        %\n",
      "10   Party Supplies                 15,046       4.4         % 87.3        %\n",
      "11   Crafts & DIY                   11,901       3.4         % 90.7        %\n",
      "12   Garden & Outdoor               11,364       3.3         % 94.0        %\n",
      "13   Games & Puzzles                5,423        1.6         % 95.6        %\n",
      "14   Lighting                       4,717        1.4         % 97.0        %\n",
      "15   Electronics & Gadgets          3,384        1.0         % 97.9        %\n",
      "16   Bath & Body                    3,290        1.0         % 98.9        %\n",
      "17   Gifts & Special Occasion       1,899        0.6         % 99.4        %\n",
      "18   Pets & Animals                 1,338        0.4         % 99.8        %\n",
      "19   Books & Magazines              571          0.2         % 100.0       %\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "CATEGORY SIZE ANALYSIS:\n",
      "  Large categories (≥10%): 2 categories, 129,831 items (37.6%)\n",
      "  Medium categories (5-10%): 4 categories, 108,198 items (31.4%)\n",
      "  Small categories (1-5%): 8 categories, 96,547 items (28.0%)\n",
      "  Tiny categories (<1%): 5 categories, 10,482 items (3.0%)\n",
      "\n",
      "Creating bar chart for top 10 categories...\n",
      "Category distribution plot saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\category_distribution_Last_Two_Thirds.png\n",
      "\n",
      "Completed processing for Last_Two_Thirds\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "PRODUCT ANALYSIS COMPLETED SUCCESSFULLY!\n",
      "All plots saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\n",
      "Generated 6 visualization files\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PRODUCT ANALYSIS SCRIPT STARTED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ===== VISUALIZATION SETTINGS =====\n",
    "print(\"\\nSetting up visualization configuration...\")\n",
    "MAIN_COLOR = \"#1f77b4\"       \n",
    "TERTIARY_COLOR = \"#2ca02c\"  \n",
    "BACKGROUND_COLOR = \"#f5f5f5\"\n",
    "TEXT_COLOR = \"#333333\"     \n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette([MAIN_COLOR, TERTIARY_COLOR])\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['axes.facecolor'] = BACKGROUND_COLOR\n",
    "plt.rcParams['axes.edgecolor'] = TEXT_COLOR\n",
    "plt.rcParams['axes.labelcolor'] = TEXT_COLOR\n",
    "plt.rcParams['text.color'] = TEXT_COLOR\n",
    "plt.rcParams['xtick.color'] = TEXT_COLOR\n",
    "plt.rcParams['ytick.color'] = TEXT_COLOR\n",
    "print(\"Visualization settings configured successfully\")\n",
    "\n",
    "# ===== DATA LOADING =====\n",
    "print(\"\\nLoading dataset...\")\n",
    "current_dir = os.getcwd() \n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "dataset_dir = os.path.join(current_dir, \"Dataset\")\n",
    "print(f\"Dataset directory: {dataset_dir}\")\n",
    "\n",
    "excel_file = 'full_validated_dataset.xlsx'\n",
    "excel_file_path = os.path.join(dataset_dir, excel_file)\n",
    "print(f\"Excel file path: {excel_file_path}\")\n",
    "\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"ERROR: Excel file not found at {excel_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Reading Excel file...\")\n",
    "df = pd.read_excel(excel_file_path)\n",
    "print(f\"Dataset loaded successfully with {len(df)} records\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "\n",
    "print(f\"\\nFiltering out 'Miscellaneous' category...\")\n",
    "original_count = len(df)\n",
    "df = df[df['category'] != 'Miscellaneous']\n",
    "filtered_count = len(df)\n",
    "print(f\"Original records: {original_count:,}\")\n",
    "print(f\"After filtering: {filtered_count:,}\")\n",
    "print(f\"Removed {original_count - filtered_count:,} records ({((original_count - filtered_count)/original_count)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "print(f\"\\nChecking Date column...\")\n",
    "if 'Date' in df.columns:\n",
    "    print(\"Date column found\")\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
    "        print(\"Converting Date column to datetime...\")\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        print(\"Date conversion completed\")\n",
    "    else:\n",
    "        print(\"Date column already in datetime format\")\n",
    "else:\n",
    "    print(\"No Date column found in dataset\")\n",
    "\n",
    "\n",
    "print(f\"\\nSetting up plots directory...\")\n",
    "plots_dir = os.path.join(dataset_dir, 'Analysis_plots')\n",
    "print(f\"Plots directory: {plots_dir}\")\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "    print(\"Plots directory created\")\n",
    "else:\n",
    "    print(\"Plots directory already exists\")\n",
    "\n",
    "# ===== DATA PARTITIONING =====\n",
    "print(f\"\\nPartitioning dataset...\")\n",
    "n = len(df)\n",
    "one_third_index = n // 3\n",
    "print(f\"Total records: {n:,}\")\n",
    "print(f\"One-third index: {one_third_index:,}\")\n",
    "\n",
    "df_whole = df.copy()\n",
    "df_first_third = df.iloc[:one_third_index]\n",
    "df_last_two_thirds = df.iloc[one_third_index:]\n",
    "\n",
    "print(f\"Whole dataset: {len(df_whole):,} records\")\n",
    "print(f\"First third: {len(df_first_third):,} records\")\n",
    "print(f\"Last two thirds: {len(df_last_two_thirds):,} records\")\n",
    "\n",
    "partitions = {\n",
    "    \"Whole_Dataset\": df_whole,\n",
    "    \"First_Third\": df_first_third,\n",
    "    \"Last_Two_Thirds\": df_last_two_thirds\n",
    "}\n",
    "print(f\"Created {len(partitions)} data partitions\")\n",
    "\n",
    "# ===== PRODUCT ANALYSIS FUNCTIONS =====\n",
    "def plot_top_products(dataframe, partition_name, save_dir):\n",
    "    \"\"\"\n",
    "    Plot the top 10 products by frequency for the given partition.\n",
    "    The plot is saved as 'top_products_{partition_name}.png'.\n",
    "    \"\"\"\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"PROCESSING TOP PRODUCTS FOR: {partition_name.upper()}\")\n",
    "    print(f\"=\"*70)\n",
    "    print(f\"Analyzing {len(dataframe):,} records...\")\n",
    "    \n",
    "  \n",
    "    all_products = dataframe['Itemname'].value_counts()\n",
    "    top_products = all_products.head(10)\n",
    "    total_items = len(dataframe)\n",
    "    unique_products = dataframe['Itemname'].nunique()\n",
    "    \n",
    "    print(f\"\\nOVERALL STATISTICS:\")\n",
    "    print(f\"  Total items in partition: {total_items:,}\")\n",
    "    print(f\"  Unique products in partition: {unique_products:,}\")\n",
    "    print(f\"  Coverage of top 10: {top_products.sum():,} items ({(top_products.sum()/total_items)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTOP 10 PRODUCTS BREAKDOWN:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Rank':<4} {'Product Name':<40} {'Count':<10} {'Percentage':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, (product, count) in enumerate(top_products.items(), 1):\n",
    "        percentage = (count / total_items) * 100\n",
    "        product_truncated = product[:37] + \"...\" if len(product) > 40 else product\n",
    "        print(f\"{i:<4} {product_truncated:<40} {count:<10} {percentage:<12.2f}%\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Top product: '{top_products.index[0]}' with {top_products.iloc[0]:,} occurrences ({(top_products.iloc[0]/total_items)*100:.1f}%)\")\n",
    "    print(f\"Least frequent in top 10: '{top_products.index[-1]}' with {top_products.iloc[-1]:,} occurrences ({(top_products.iloc[-1]/total_items)*100:.1f}%)\")\n",
    "    \n",
    "\n",
    "    remaining_products = unique_products - 10\n",
    "    remaining_items = total_items - top_products.sum()\n",
    "    if remaining_products > 0:\n",
    "        avg_frequency_remaining = remaining_items / remaining_products\n",
    "        print(f\"Remaining {remaining_products:,} products: {remaining_items:,} items (avg {avg_frequency_remaining:.1f} per product)\")\n",
    "    \n",
    "\n",
    "    print(f\"\\nCreating horizontal bar chart...\")\n",
    "    plt.figure(figsize=(14, 10), facecolor=BACKGROUND_COLOR)\n",
    "    \n",
    "    bars = plt.barh(top_products.index[::-1], top_products.values[::-1], color=MAIN_COLOR, edgecolor='white')\n",
    "    plt.title(f\"Top 10 Products by Frequency - {partition_name.replace('_', ' ')}\", \n",
    "              fontweight='bold', fontsize=16, color=TEXT_COLOR)\n",
    "    plt.xlabel(\"Number of Occurrences\", fontsize=14, color=TEXT_COLOR)\n",
    "    plt.ylabel(\"Product Name\", fontsize=14, color=TEXT_COLOR)\n",
    "    \n",
    "\n",
    "    for i, bar in enumerate(bars):\n",
    "        value = top_products.values[::-1][i]\n",
    "        percentage = (value / total_items) * 100\n",
    "        plt.text(value + 10, bar.get_y() + bar.get_height()/2, \n",
    "                 f\"{value} ({percentage:.1f}%)\", va='center', fontsize=10, color=TEXT_COLOR)\n",
    "    \n",
    "    plt.grid(axis='x', alpha=0.3, color=\"#cccccc\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"top_products_{partition_name}.png\")\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Top products plot saved to: {save_path}\")\n",
    "\n",
    "def plot_category_distribution(dataframe, partition_name, save_dir):\n",
    "    \"\"\"\n",
    "    Plot the product category distribution for the given partition if 'category' column exists.\n",
    "    The plot is saved as 'category_distribution_{partition_name}.png'.\n",
    "    \"\"\"\n",
    "    if 'category' not in dataframe.columns:\n",
    "        print(f\"No 'category' column found for {partition_name}. Skipping category distribution plot.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"PROCESSING CATEGORY DISTRIBUTION FOR: {partition_name.upper()}\")\n",
    "    print(f\"=\"*70)\n",
    "    print(f\"Analyzing {len(dataframe):,} records...\")\n",
    "    \n",
    " \n",
    "    all_categories = dataframe['category'].value_counts()\n",
    "    total_items = len(dataframe)\n",
    "    unique_categories = dataframe['category'].nunique()\n",
    "    \n",
    "    print(f\"\\nOVERALL CATEGORY STATISTICS:\")\n",
    "    print(f\"  Total items in partition: {total_items:,}\")\n",
    "    print(f\"  Unique categories in partition: {unique_categories}\")\n",
    "    print(f\"  Most popular category: '{all_categories.index[0]}' with {all_categories.iloc[0]:,} items ({(all_categories.iloc[0]/total_items)*100:.1f}%)\")\n",
    "    print(f\"  Least popular category: '{all_categories.index[-1]}' with {all_categories.iloc[-1]:,} items ({(all_categories.iloc[-1]/total_items)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nCOMPLETE CATEGORY BREAKDOWN:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Rank':<4} {'Category Name':<30} {'Count':<12} {'Percentage':<12} {'Cumulative %':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    cumulative_percentage = 0\n",
    "    for i, (category, count) in enumerate(all_categories.items(), 1):\n",
    "        percentage = (count / total_items) * 100\n",
    "        cumulative_percentage += percentage\n",
    "        category_truncated = category[:27] + \"...\" if len(category) > 30 else category\n",
    "        print(f\"{i:<4} {category_truncated:<30} {count:<12,} {percentage:<12.1f}% {cumulative_percentage:<12.1f}%\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    " \n",
    "    print(f\"\\nCATEGORY SIZE ANALYSIS:\")\n",
    "    large_categories = all_categories[all_categories >= total_items * 0.1]  # Categories with 10%+ of items\n",
    "    medium_categories = all_categories[(all_categories >= total_items * 0.05) & (all_categories < total_items * 0.1)]  # 5-10%\n",
    "    small_categories = all_categories[(all_categories >= total_items * 0.01) & (all_categories < total_items * 0.05)]  # 1-5%\n",
    "    tiny_categories = all_categories[all_categories < total_items * 0.01]  # <1%\n",
    "    \n",
    "    print(f\"  Large categories (≥10%): {len(large_categories)} categories, {large_categories.sum():,} items ({(large_categories.sum()/total_items)*100:.1f}%)\")\n",
    "    print(f\"  Medium categories (5-10%): {len(medium_categories)} categories, {medium_categories.sum():,} items ({(medium_categories.sum()/total_items)*100:.1f}%)\")\n",
    "    print(f\"  Small categories (1-5%): {len(small_categories)} categories, {small_categories.sum():,} items ({(small_categories.sum()/total_items)*100:.1f}%)\")\n",
    "    print(f\"  Tiny categories (<1%): {len(tiny_categories)} categories, {tiny_categories.sum():,} items ({(tiny_categories.sum()/total_items)*100:.1f}%)\")\n",
    "    \n",
    "\n",
    "    category_counts = all_categories.head(10)\n",
    "    \n",
    " \n",
    "    print(f\"\\nCreating bar chart for top 10 categories...\")\n",
    "    plt.figure(figsize=(14, 8), facecolor=BACKGROUND_COLOR)\n",
    "    bars = plt.bar(category_counts.index, category_counts.values, color=MAIN_COLOR, edgecolor='white')\n",
    "    plt.title(f\"Product Categories Distribution - {partition_name.replace('_', ' ')}\", \n",
    "              fontweight='bold', fontsize=16, color=TEXT_COLOR)\n",
    "    plt.xlabel(\"Category\", fontsize=14, color=TEXT_COLOR)\n",
    "    plt.ylabel(\"Number of Items\", fontsize=14, color=TEXT_COLOR)\n",
    "    plt.xticks(rotation=45, ha='right', color=TEXT_COLOR)\n",
    "    \n",
    "\n",
    "    for i, bar in enumerate(bars):\n",
    "        value = category_counts.values[i]\n",
    "        percentage = (value / total_items) * 100\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, value + 5, \n",
    "                 f\"{value:,}\\n({percentage:.1f}%)\", ha='center', va='bottom', \n",
    "                 fontsize=10, color=TEXT_COLOR, fontweight='bold')\n",
    "    \n",
    "    plt.grid(axis='y', alpha=0.3, color=\"#cccccc\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"category_distribution_{partition_name}.png\")\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Category distribution plot saved to: {save_path}\")\n",
    "\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING COMPREHENSIVE ANALYSIS FOR ALL PARTITIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (partition_name, partition_df) in enumerate(partitions.items(), 1):\n",
    "    print(f\"\\n[{i}/{len(partitions)}] PROCESSING PARTITION: {partition_name.upper()}\")\n",
    "    print(f\"Partition size: {len(partition_df):,} records\")\n",
    "    print(f\"Date range: {partition_df['Date'].min()} to {partition_df['Date'].max()}\" if 'Date' in partition_df.columns else \"No date information\")\n",
    "    \n",
    "\n",
    "    plot_top_products(partition_df, partition_name, plots_dir)\n",
    "    \n",
    "\n",
    "    plot_category_distribution(partition_df, partition_name, plots_dir)\n",
    "    \n",
    "    print(f\"\\nCompleted processing for {partition_name}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"PRODUCT ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"All plots saved to: {plots_dir}\")\n",
    "print(f\"Generated {len(partitions) * 2} visualization files\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
