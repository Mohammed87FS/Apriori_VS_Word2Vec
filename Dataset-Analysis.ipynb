{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "\n",
    "- **How many rows**: Number of roows in the dataset.\n",
    "- **How many columns**: Number of columns in the dataset.\n",
    "- **List of the columns**: List of columns in the dataset.\n",
    "- **Total Products**: Number of products in the dataset.\n",
    "- **Distinct Products**: Number of unique products.\n",
    "- **Total Transactions**: Number of transactions recorded.\n",
    "- **Total Customers**: Number of customers in the dataset.\n",
    "- **Distinct Categories**: Number of unique product categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DIAGNOSTIC ANALYSIS ===\n",
      "df1 shape: (520609, 8)\n",
      "df2 shape: (4185, 2)\n",
      "\n",
      "df1 columns: ['BillNo', 'Itemname', 'Quantity', 'Date', 'Price', 'CustomerID', 'Country', 'category']\n",
      "df2 columns: ['Itemname', 'Corrected_Category']\n",
      "\n",
      "Null values in df1['Itemname']: 0\n",
      "Null values in df2['Itemname']: 0\n",
      "\n",
      "Unique items in df1: 4185\n",
      "Unique items in df2: 4185\n",
      "\n",
      "Items in df1 but NOT in df2: 3\n",
      "Items in df2 but NOT in df1: 3\n",
      "\n",
      "================================================================================\n",
      "ALL ITEMS IN DF1 BUT NOT IN DF2 (3 items):\n",
      "================================================================================\n",
      " 1. 'OOPS ! adjustment'\n",
      " 2. 'Wrongly mrked had 85123a in box'\n",
      " 3. 'crushed ctn'\n",
      "\n",
      "================================================================================\n",
      "ALL ITEMS IN DF2 BUT NOT IN DF1 (3 items):\n",
      "================================================================================\n",
      " 1. 'OOPS ! Adjustment'\n",
      " 2. 'crushed ctn HAPPY STENCIL CRAFT'\n",
      " 3. 'wrongly mrked had 85123a in box'\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS OF UNMATCHED ITEMS FROM DF1:\n",
      "================================================================================\n",
      "Total rows affected by unmatched items: 3\n",
      "\n",
      "Top 10 most frequent unmatched items in df1:\n",
      "  'crushed ctn': 1 occurrences\n",
      "  'OOPS ! adjustment': 1 occurrences\n",
      "  'Wrongly mrked had 85123a in box': 1 occurrences\n",
      "\n",
      "=== STRING ANALYSIS ===\n",
      "Sample items from df1:\n",
      "  'WHITE HANGING HEART T-LIGHT HOLDER' (length: 34)\n",
      "  'WHITE METAL LANTERN' (length: 19)\n",
      "  'CREAM CUPID HEARTS COAT HANGER' (length: 30)\n",
      "  'KNITTED UNION FLAG HOT WATER BOTTLE' (length: 35)\n",
      "  'RED WOOLLY HOTTIE WHITE HEART.' (length: 30)\n",
      "\n",
      "Sample items from df2:\n",
      "  'APPLE BATH SPONGE' (length: 17)\n",
      "  'BATHROOM SCALES FOOTPRINTS IN SAND' (length: 34)\n",
      "  'BATHROOM SCALES RUBBER DUCKS' (length: 28)\n",
      "  'BATHROOM SCALES, TROPICAL BEACH' (length: 31)\n",
      "  'BATHROOM SET LOVE HEART DESIGN' (length: 30)\n",
      "\n",
      "=== MERGE RESULTS ===\n",
      "Original df1 rows: 520609\n",
      "Merged rows: 520609\n",
      "Rows with NO data from df2: 3\n",
      "Rows with data from df2: 520606\n",
      "\n",
      "First 10 items that didn't get matched:\n",
      "  'crushed ctn'\n",
      "  'OOPS ! adjustment'\n",
      "  'Wrongly mrked had 85123a in box'\n",
      "\n",
      "Merged dataset saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\full_validated_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd() \n",
    "dataset_dir = os.path.join(current_dir, \"Dataset\")\n",
    "\n",
    "excel_file = 'not_validated_dataset_with_category.xlsx'\n",
    "excel_file_path = os.path.join(dataset_dir, excel_file)\n",
    "\n",
    "excel_file2 = 'validated_distinct_products_with_categories.xlsx'\n",
    "excel_file_path2 = os.path.join(dataset_dir, excel_file2)\n",
    "\n",
    "df1 = pd.read_excel(excel_file_path)\n",
    "df2 = pd.read_excel(excel_file_path2)\n",
    "\n",
    "print(\"=== DIAGNOSTIC ANALYSIS ===\")\n",
    "print(f\"df1 shape: {df1.shape}\")\n",
    "print(f\"df2 shape: {df2.shape}\")\n",
    "\n",
    "\n",
    "print(f\"\\ndf1 columns: {list(df1.columns)}\")\n",
    "print(f\"df2 columns: {list(df2.columns)}\")\n",
    "\n",
    "\n",
    "if 'Itemname' not in df1.columns:\n",
    "    print(\" ERROR: 'Itemname' column not found in df1!\")\n",
    "if 'Itemname' not in df2.columns:\n",
    "    print(\" ERROR: 'Itemname' column not found in df2!\")\n",
    "\n",
    "\n",
    "print(f\"\\nNull values in df1['Itemname']: {df1['Itemname'].isnull().sum()}\")\n",
    "print(f\"Null values in df2['Itemname']: {df2['Itemname'].isnull().sum()}\")\n",
    "\n",
    "\n",
    "df1_items = set(df1['Itemname'].dropna())\n",
    "df2_items = set(df2['Itemname'].dropna())\n",
    "\n",
    "print(f\"\\nUnique items in df1: {len(df1_items)}\")\n",
    "print(f\"Unique items in df2: {len(df2_items)}\")\n",
    "\n",
    "\n",
    "items_in_df1_not_in_df2 = df1_items - df2_items\n",
    "items_in_df2_not_in_df1 = df2_items - df1_items\n",
    "\n",
    "print(f\"\\nItems in df1 but NOT in df2: {len(items_in_df1_not_in_df2)}\")\n",
    "print(f\"Items in df2 but NOT in df1: {len(items_in_df2_not_in_df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL ITEMS IN DF1 BUT NOT IN DF2 ({len(items_in_df1_not_in_df2)} items):\")\n",
    "print(f\"{'='*80}\")\n",
    "for i, item in enumerate(sorted(items_in_df1_not_in_df2), 1):\n",
    "    print(f\"{i:2d}. '{item}'\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL ITEMS IN DF2 BUT NOT IN DF1 ({len(items_in_df2_not_in_df1)} items):\")\n",
    "print(f\"{'='*80}\")\n",
    "for i, item in enumerate(sorted(items_in_df2_not_in_df1), 1):\n",
    "    print(f\"{i:2d}. '{item}'\")\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ANALYSIS OF UNMATCHED ITEMS FROM DF1:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "df1_unmatched_counts = df1[df1['Itemname'].isin(items_in_df1_not_in_df2)]['Itemname'].value_counts()\n",
    "print(f\"Total rows affected by unmatched items: {df1_unmatched_counts.sum()}\")\n",
    "print(\"\\nTop 10 most frequent unmatched items in df1:\")\n",
    "for item, count in df1_unmatched_counts.head(10).items():\n",
    "    print(f\"  '{item}': {count} occurrences\")\n",
    "\n",
    "unmatched_df1 = pd.DataFrame({\n",
    "    'Itemname': list(items_in_df1_not_in_df2),\n",
    "    'Frequency_in_df1': [df1['Itemname'].value_counts().get(item, 0) for item in items_in_df1_not_in_df2]\n",
    "}).sort_values('Frequency_in_df1', ascending=False)\n",
    "\n",
    "unmatched_df2 = pd.DataFrame({\n",
    "    'Itemname': list(items_in_df2_not_in_df1)\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n=== STRING ANALYSIS ===\")\n",
    "print(\"Sample items from df1:\")\n",
    "for item in df1['Itemname'].dropna().head(5):\n",
    "    print(f\"  '{item}' (length: {len(str(item))})\")\n",
    "\n",
    "print(\"\\nSample items from df2:\")\n",
    "for item in df2['Itemname'].dropna().head(5):\n",
    "    print(f\"  '{item}' (length: {len(str(item))})\")\n",
    "\n",
    "\n",
    "merged_df = df1.merge(df2, on='Itemname', how='left')\n",
    "\n",
    "\n",
    "print(f\"\\n=== MERGE RESULTS ===\")\n",
    "print(f\"Original df1 rows: {len(df1)}\")\n",
    "print(f\"Merged rows: {len(merged_df)}\")\n",
    "\n",
    "\n",
    "df2_columns = [col for col in df2.columns if col != 'Itemname']\n",
    "if df2_columns:\n",
    "    missing_data_mask = merged_df[df2_columns].isnull().all(axis=1)\n",
    "    rows_with_missing_data = missing_data_mask.sum()\n",
    "    print(f\"Rows with NO data from df2: {rows_with_missing_data}\")\n",
    "    print(f\"Rows with data from df2: {len(merged_df) - rows_with_missing_data}\")\n",
    "    \n",
    "    \n",
    "    if rows_with_missing_data > 0:\n",
    "        print(f\"\\nFirst 10 items that didn't get matched:\")\n",
    "        unmatched_items = merged_df[missing_data_mask]['Itemname'].unique()[:10]\n",
    "        for item in unmatched_items:\n",
    "            print(f\"  '{item}'\")\n",
    "\n",
    "\n",
    "output_file = 'full_validated_dataset.xlsx'\n",
    "output_file_path = os.path.join(dataset_dir, output_file)\n",
    "merged_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"\\nMerged dataset saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing Miscellaneous category: 519855 records\n",
      "\n",
      " ===== DATA ANALYSIS ===== \n",
      "\n",
      "Dataset Overview (Whole Dataset):\n",
      "==================================================\n",
      "  Rows: 519,855\n",
      "  Columns: 9\n",
      "  Column Names: BillNo, Itemname, Quantity, Date, Price, CustomerID, Country, category, Corrected_Category\n",
      "==================================================\n",
      "  Total Products: 519,855\n",
      "  Distinct Products: 4,122\n",
      "  Total Transactions: 19,960\n",
      "  Total Customers: 4,297\n",
      "  Distinct Categories: 20\n",
      "==================================================\n",
      "\n",
      "Comparison Table:\n",
      "                     Full Dataset  First Third  Last Two Thirds\n",
      "Metric                                                         \n",
      "Rows                       519855       173285           346570\n",
      "Columns                         9            9                9\n",
      "Total Products             519855       173285           346570\n",
      "Distinct Products            4122         3296             3696\n",
      "Total Transactions          19960         6778            13182\n",
      "Total Customers              4297         2488             3632\n",
      "Distinct Categories            20           20               20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "BACKGROUND_COLOR = \"#f5f5f5\" \n",
    "TEXT_COLOR = \"#333333\"       \n",
    "\n",
    "\n",
    "current_dir = os.getcwd() \n",
    "dataset_dir = os.path.join(current_dir, \"Dataset\")\n",
    "\n",
    "\n",
    "excel_file = 'full_validated_dataset.xlsx'\n",
    "excel_file_path = os.path.join(dataset_dir, excel_file)\n",
    "\n",
    "# ===== DATA LOADING =====\n",
    "\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "\n",
    "df = df[df['category'] != 'Miscellaneous']\n",
    "print(f\"Dataset after removing Miscellaneous category: {len(df)} records\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ===== DATA ANALYSIS ===== \\n\")\n",
    "\n",
    "\n",
    "plots_dir = os.path.join(current_dir, 'Analysis_plots')\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "# ===== FUNCTION DEFINITION =====\n",
    "def compute_stats(dataframe):\n",
    "    \"\"\"\n",
    "    Compute key statistics for the given DataFrame.\n",
    "    Returns a dictionary with analysis results.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    stats['Rows'] = len(dataframe)\n",
    "    stats['Columns'] = len(dataframe.columns)\n",
    "    stats['Total Products'] = len(dataframe)  \n",
    "    stats['Distinct Products'] = dataframe['Itemname'].nunique()\n",
    "    stats['Total Transactions'] = dataframe['BillNo'].nunique()\n",
    "    stats['Total Customers'] = int(dataframe['CustomerID'].dropna().nunique())\n",
    "    stats['Distinct Categories'] = dataframe['category'].nunique()\n",
    "    return stats\n",
    "\n",
    "# ===== ANALYSIS FOR DIFFERENT PARTITIONS =====\n",
    "\n",
    "stats_whole = compute_stats(df)\n",
    "\n",
    "\n",
    "one_third_index = len(df) // 3\n",
    "\n",
    "df_first_third = df.iloc[:one_third_index]\n",
    "df_last_two_thirds = df.iloc[one_third_index:]\n",
    "\n",
    "stats_first_third = compute_stats(df_first_third)\n",
    "stats_last_two_thirds = compute_stats(df_last_two_thirds)\n",
    "\n",
    "# ===== PRINT OVERVIEW FOR WHOLE DATASET =====\n",
    "print(\"Dataset Overview (Whole Dataset):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Rows: {stats_whole['Rows']:,}\")\n",
    "print(f\"  Columns: {stats_whole['Columns']}\")\n",
    "print(f\"  Column Names: {', '.join(df.columns.tolist())}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Total Products: {stats_whole['Total Products']:,}\")\n",
    "print(f\"  Distinct Products: {stats_whole['Distinct Products']:,}\")\n",
    "print(f\"  Total Transactions: {stats_whole['Total Transactions']:,}\")\n",
    "print(f\"  Total Customers: {stats_whole['Total Customers']:,}\")\n",
    "print(f\"  Distinct Categories: {stats_whole['Distinct Categories']}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ===== COMPARISON TABLE =====\n",
    "\n",
    "comparison_data = {\n",
    "    \"Full Dataset\": stats_whole,\n",
    "    \"First Third\": stats_first_third,\n",
    "    \"Last Two Thirds\": stats_last_two_thirds\n",
    "}\n",
    "comparison_table = pd.DataFrame(comparison_data)\n",
    "comparison_table.index.name = \"Metric\"\n",
    "\n",
    "print(\"\\nComparison Table:\")\n",
    "print(comparison_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing Miscellaneous category: 519855 records\n",
      "Excel file saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\distinct_products_with_categories.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd() \n",
    "dataset_dir = os.path.join(current_dir, \"Dataset\")\n",
    "\n",
    "\n",
    "excel_file = 'full_validated_dataset.xlsx'\n",
    "excel_file_path = os.path.join(dataset_dir, excel_file)\n",
    "\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "data_excel = load_dataset(excel_file_path)\n",
    "\n",
    "data_excel = data_excel[data_excel['category'] != 'Miscellaneous']\n",
    "print(f\"Dataset after removing Miscellaneous category: {len(data_excel)} records\")\n",
    "\n",
    "\n",
    "\n",
    "data_excel.dropna(subset=['Itemname'], inplace=True)   \n",
    "output_file = os.path.join(current_dir, 'unique_items.xlsx')\n",
    "\n",
    "unique_items_df = pd.DataFrame({'unique_items': data_excel['Itemname'].unique()})\n",
    "unique_items_df.to_excel(output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "distinct_products_df = data_excel[['Itemname', 'category']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "distinct_products_df = distinct_products_df.sort_values(['category', 'Itemname'])\n",
    "\n",
    "\n",
    "output_file = os.path.join(dataset_dir, 'distinct_products_with_categories.xlsx')\n",
    "distinct_products_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Excel file saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Patterns\n",
    "\n",
    "Analyze the purchasing patterns visible in the data:\n",
    "\n",
    "- **Average items per transaction**.\n",
    "- **Distribution of transaction sizes**: Histogram showing the number of items per invoice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing Miscellaneous category: 519855 records\n",
      "\n",
      " ===== TRANSACTION PATTERNS ANALYSIS ===== \n",
      "\n",
      "Average Items per Transaction (Whole Dataset):\n",
      "==================================================\n",
      " Average Items: 26.04\n",
      " Median Items: 15.0\n",
      " Min Items: 1\n",
      " Max Items: 1113\n",
      "==================================================\n",
      "\n",
      "Transaction Size Percentiles (Whole Dataset):\n",
      " 25th percentile: 6.0 items\n",
      " 50th percentile (median): 15.0 items\n",
      " 75th percentile: 29.0 items\n",
      " 90th percentile: 53.0 items\n",
      " 95th percentile: 78.0 items\n",
      " 99th percentile: 219.0 items\n",
      "==================================================\n",
      "\n",
      "Comparison Table (Transaction Metrics):\n",
      "                   Whole  First Third  Last Two Thirds\n",
      "Metric                                                \n",
      "Avg Items       26.04484    25.565801        26.291155\n",
      "Median Items    15.00000    15.000000        15.000000\n",
      "Min Items        1.00000     1.000000         1.000000\n",
      "Max Items     1113.00000   674.000000      1113.000000\n",
      "P25              6.00000     6.000000         6.000000\n",
      "P50             15.00000    15.000000        15.000000\n",
      "P75             29.00000    28.000000        29.000000\n",
      "P90             53.00000    52.000000        54.000000\n",
      "P95             78.00000    73.000000        80.000000\n",
      "P99            219.00000   217.230000       219.190000\n",
      "\n",
      "Comparison table saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\transaction_comparison_table.xlsx\n",
      "Plot saved for Whole Dataset at: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\transaction_size_distribution_Whole_Dataset.png\n",
      "Plot saved for First Third at: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\transaction_size_distribution_First_Third.png\n",
      "Plot saved for Last Two Thirds at: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\transaction_size_distribution_Last_Two_Thirds.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# ===== VISUALIZATION SETTINGS =====\n",
    "MAIN_COLOR = \"#1f77b4\"       \n",
    "TERTIARY_COLOR = \"#2ca02c\"   \n",
    "BACKGROUND_COLOR = \"#f5f5f5\"\n",
    "TEXT_COLOR = \"#333333\"       \n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette([MAIN_COLOR, TERTIARY_COLOR])\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['axes.facecolor'] = BACKGROUND_COLOR\n",
    "plt.rcParams['axes.edgecolor'] = TEXT_COLOR\n",
    "plt.rcParams['axes.labelcolor'] = TEXT_COLOR\n",
    "plt.rcParams['text.color'] = TEXT_COLOR\n",
    "plt.rcParams['xtick.color'] = TEXT_COLOR\n",
    "plt.rcParams['ytick.color'] = TEXT_COLOR\n",
    "\n",
    "# ===== DATA LOADING =====\n",
    "current_dir = os.getcwd() \n",
    "dataset_dir = os.path.join(current_dir, \"Dataset\")\n",
    "excel_file = 'full_validated_dataset.xlsx'\n",
    "excel_file_path = os.path.join(dataset_dir, excel_file)\n",
    "\n",
    "\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "df = df[df['category'] != 'Miscellaneous']\n",
    "print(f\"Dataset after removing Miscellaneous category: {len(df)} records\")\n",
    "\n",
    "\n",
    "if 'Date' in df.columns:\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "plots_dir = os.path.join(dataset_dir, 'Analysis_plots')\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "# ===== HELPER FUNCTION FOR TRANSACTION ANALYSIS =====\n",
    "def compute_transaction_metrics(dataframe):\n",
    "    \"\"\"\n",
    "    Compute transaction metrics:\n",
    "      - items per transaction: average, median, min, max\n",
    "      - transaction size percentiles (25th, 50th, 75th, 90th, 95th, 99th)\n",
    "    Returns a tuple with (metrics dictionary, items_per_transaction series)\n",
    "    \"\"\"\n",
    "    \n",
    "    items_per_transaction = dataframe.groupby('BillNo').size()\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['Avg Items'] = items_per_transaction.mean()\n",
    "    metrics['Median Items'] = items_per_transaction.median()\n",
    "    metrics['Min Items'] = items_per_transaction.min()\n",
    "    metrics['Max Items'] = items_per_transaction.max()\n",
    "    \n",
    "    \n",
    "    percentiles = np.percentile(items_per_transaction, [25, 50, 75, 90, 95, 99])\n",
    "    metrics['P25'] = percentiles[0]\n",
    "    metrics['P50'] = percentiles[1] \n",
    "    metrics['P75'] = percentiles[2]\n",
    "    metrics['P90'] = percentiles[3]\n",
    "    metrics['P95'] = percentiles[4]\n",
    "    metrics['P99'] = percentiles[5]\n",
    "    \n",
    "    return metrics, items_per_transaction\n",
    "\n",
    "# ===== DATA PARTITIONING =====\n",
    "\n",
    "n = len(df)\n",
    "one_third_index = n // 3\n",
    "\n",
    "df_whole = df.copy()\n",
    "df_first_third = df.iloc[:one_third_index]\n",
    "df_last_two_thirds = df.iloc[one_third_index:]\n",
    "\n",
    "# ===== ANALYZE TRANSACTION PATTERNS =====\n",
    "print(\"\\n ===== TRANSACTION PATTERNS ANALYSIS ===== \\n\")\n",
    "\n",
    "\n",
    "metrics_whole, items_whole = compute_transaction_metrics(df_whole)\n",
    "metrics_first_third, items_first_third = compute_transaction_metrics(df_first_third)\n",
    "metrics_last_two_thirds, items_last_two_thirds = compute_transaction_metrics(df_last_two_thirds)\n",
    "\n",
    "\n",
    "print(\"Average Items per Transaction (Whole Dataset):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\" Average Items: {metrics_whole['Avg Items']:.2f}\")\n",
    "print(f\" Median Items: {metrics_whole['Median Items']}\")\n",
    "print(f\" Min Items: {metrics_whole['Min Items']}\")\n",
    "print(f\" Max Items: {metrics_whole['Max Items']}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTransaction Size Percentiles (Whole Dataset):\")\n",
    "print(f\" 25th percentile: {metrics_whole['P25']:.1f} items\")\n",
    "print(f\" 50th percentile (median): {metrics_whole['P50']:.1f} items\")\n",
    "print(f\" 75th percentile: {metrics_whole['P75']:.1f} items\")\n",
    "print(f\" 90th percentile: {metrics_whole['P90']:.1f} items\")\n",
    "print(f\" 95th percentile: {metrics_whole['P95']:.1f} items\")\n",
    "print(f\" 99th percentile: {metrics_whole['P99']:.1f} items\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ===== COMPARISON TABLE =====\n",
    "comparison_data = {\n",
    "    \"Whole\": metrics_whole,\n",
    "    \"First Third\": metrics_first_third,\n",
    "    \"Last Two Thirds\": metrics_last_two_thirds\n",
    "}\n",
    "comparison_table = pd.DataFrame(comparison_data)\n",
    "comparison_table.index.name = \"Metric\"\n",
    "print(\"\\nComparison Table (Transaction Metrics):\")\n",
    "print(comparison_table)\n",
    "\n",
    "\n",
    "comparison_table_file = os.path.join(plots_dir, \"transaction_comparison_table.xlsx\")\n",
    "comparison_table.to_excel(comparison_table_file)\n",
    "print(f\"\\nComparison table saved to: {comparison_table_file}\")\n",
    "\n",
    "# ===== PLOTTING FUNCTION =====\n",
    "def plot_transaction_distribution(items_series, title, save_path):\n",
    "    \"\"\"\n",
    "    Plot the transaction size distribution given a series of items per transaction.\n",
    "    Saves the plot to the provided save_path.\n",
    "    \"\"\"\n",
    "  \n",
    "    size_bins = [1, 5, 10, 15, 20, 30, 50, 100, np.inf]\n",
    "    size_labels = ['1-4', '5-9', '10-14', '15-19', '20-29', '30-49', '50-99', '100+']\n",
    "    \n",
    " \n",
    "    transaction_binned = pd.cut(items_series, bins=size_bins, labels=size_labels)\n",
    "    size_counts = transaction_binned.value_counts().sort_index()\n",
    "    total_transactions = len(items_series)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(12, 7), facecolor=BACKGROUND_COLOR)\n",
    "    bars = plt.bar(size_counts.index.astype(str), size_counts.values, color=MAIN_COLOR, edgecolor='white')\n",
    "    plt.title(f'Transaction Size Distribution - {title}', fontweight='bold', fontsize=16, color=TEXT_COLOR)\n",
    "    plt.xlabel('Number of Items', fontsize=14, color=TEXT_COLOR)\n",
    "    plt.ylabel('Transactions', fontsize=14, color=TEXT_COLOR)\n",
    "    \n",
    "\n",
    "    for i, v in enumerate(size_counts.values):\n",
    "        percentage = (v / total_transactions) * 100\n",
    "        plt.text(i, v + 0.05 * total_transactions, f\"{percentage:.1f}%\", ha='center', fontsize=10, \n",
    "                 fontweight='bold', color=TEXT_COLOR)\n",
    "    \n",
    "  \n",
    "    max_bin = size_counts.idxmax()\n",
    "    max_idx = list(size_counts.index).index(max_bin)\n",
    "    bars[max_idx].set_edgecolor('black')\n",
    "    bars[max_idx].set_linewidth(2)\n",
    "    \n",
    "    plt.grid(axis='y', alpha=0.3, color=\"#cccccc\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()  \n",
    "\n",
    "# ===== GENERATE AND SAVE SEPARATE PLOTS =====\n",
    "\n",
    "partitions = {\n",
    "    \"Whole_Dataset\": items_whole,\n",
    "    \"First_Third\": items_first_third,\n",
    "    \"Last_Two_Thirds\": items_last_two_thirds\n",
    "}\n",
    "\n",
    "for partition_name, items_series in partitions.items():\n",
    "    plot_title = partition_name.replace(\"_\", \" \")\n",
    "    save_file = os.path.join(plots_dir, f\"transaction_size_distribution_{partition_name}.png\")\n",
    "    plot_transaction_distribution(items_series, plot_title, save_file)\n",
    "    print(f\"Plot saved for {plot_title} at: {save_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products Analysis\n",
    "\n",
    "Examine the product-related characteristics:\n",
    "\n",
    "- **Top 10 most frequently purchased products**.\n",
    "- **Product category distribution**: Percentage of items in each category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing Miscellaneous category: 519855 records\n",
      "\n",
      "Processing Top Products for: Whole_Dataset\n",
      "Top products plot saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\top_products_Whole_Dataset.png\n",
      "\n",
      "Processing Category Distribution for: Whole_Dataset\n",
      "Category distribution plot saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\category_distribution_Whole_Dataset.png\n",
      "\n",
      "Processing Top Products for: First_Third\n",
      "Top products plot saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\top_products_First_Third.png\n",
      "\n",
      "Processing Category Distribution for: First_Third\n",
      "Category distribution plot saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\category_distribution_First_Third.png\n",
      "\n",
      "Processing Top Products for: Last_Two_Thirds\n",
      "Top products plot saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\top_products_Last_Two_Thirds.png\n",
      "\n",
      "Processing Category Distribution for: Last_Two_Thirds\n",
      "Category distribution plot saved to: c:\\Users\\moham\\Coding-Projects\\Apriori_VS_Word2Vec\\Dataset\\Analysis_plots\\category_distribution_Last_Two_Thirds.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# ===== VISUALIZATION SETTINGS =====\n",
    "MAIN_COLOR = \"#1f77b4\"       # Primary blue\n",
    "TERTIARY_COLOR = \"#2ca02c\"   # Green for additional elements\n",
    "BACKGROUND_COLOR = \"#f5f5f5\" # Light gray background\n",
    "TEXT_COLOR = \"#333333\"       # Dark gray for text\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette([MAIN_COLOR, TERTIARY_COLOR])\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['axes.facecolor'] = BACKGROUND_COLOR\n",
    "plt.rcParams['axes.edgecolor'] = TEXT_COLOR\n",
    "plt.rcParams['axes.labelcolor'] = TEXT_COLOR\n",
    "plt.rcParams['text.color'] = TEXT_COLOR\n",
    "plt.rcParams['xtick.color'] = TEXT_COLOR\n",
    "plt.rcParams['ytick.color'] = TEXT_COLOR\n",
    "\n",
    "# ===== DATA LOADING =====\n",
    "current_dir = os.getcwd() \n",
    "dataset_dir = os.path.join(current_dir, \"Dataset\")\n",
    "excel_file = 'full_validated_dataset.xlsx'\n",
    "excel_file_path = os.path.join(dataset_dir, excel_file)\n",
    "\n",
    "\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "df = df[df['category'] != 'Miscellaneous']\n",
    "print(f\"Dataset after removing Miscellaneous category: {len(df)} records\")\n",
    "\n",
    "\n",
    "if 'Date' in df.columns:\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "\n",
    "plots_dir = os.path.join(dataset_dir, 'Analysis_plots')\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "# ===== DATA PARTITIONING =====\n",
    "\n",
    "n = len(df)\n",
    "one_third_index = n // 3\n",
    "\n",
    "df_whole = df.copy()\n",
    "df_first_third = df.iloc[:one_third_index]\n",
    "df_last_two_thirds = df.iloc[one_third_index:]\n",
    "\n",
    "partitions = {\n",
    "    \"Whole_Dataset\": df_whole,\n",
    "    \"First_Third\": df_first_third,\n",
    "    \"Last_Two_Thirds\": df_last_two_thirds\n",
    "}\n",
    "\n",
    "# ===== PRODUCT ANALYSIS FUNCTIONS =====\n",
    "def plot_top_products(dataframe, partition_name, save_dir):\n",
    "    \"\"\"\n",
    "    Plot the top 10 products by frequency for the given partition.\n",
    "    The plot is saved as 'top_products_{partition_name}.png'.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing Top Products for: {partition_name}\")\n",
    "    top_products = dataframe['Itemname'].value_counts().head(10)\n",
    "    total_items = len(dataframe)\n",
    "    \n",
    "    plt.figure(figsize=(14, 10), facecolor=BACKGROUND_COLOR)\n",
    "  \n",
    "    bars = plt.barh(top_products.index[::-1], top_products.values[::-1], color=MAIN_COLOR, edgecolor='white')\n",
    "    plt.title(f\"Top 10 Products by Frequency - {partition_name.replace('_', ' ')}\", \n",
    "              fontweight='bold', fontsize=16, color=TEXT_COLOR)\n",
    "    plt.xlabel(\"Number of Occurrences\", fontsize=14, color=TEXT_COLOR)\n",
    "    plt.ylabel(\"Product Name\", fontsize=14, color=TEXT_COLOR)\n",
    "    \n",
    "  \n",
    "    for i, bar in enumerate(bars):\n",
    "        value = top_products.values[::-1][i]\n",
    "        plt.text(value + 10, bar.get_y() + bar.get_height()/2, \n",
    "                 f\"{value}\", va='center', fontsize=10, color=TEXT_COLOR)\n",
    "    \n",
    "    plt.grid(axis='x', alpha=0.3, color=\"#cccccc\")\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f\"top_products_{partition_name}.png\")\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Top products plot saved to: {save_path}\")\n",
    "\n",
    "def plot_category_distribution(dataframe, partition_name, save_dir):\n",
    "    \"\"\"\n",
    "    Plot the product category distribution for the given partition if 'category' column exists.\n",
    "    The plot is saved as 'category_distribution_{partition_name}.png'.\n",
    "    \"\"\"\n",
    "    if 'category' not in dataframe.columns:\n",
    "        print(f\"No 'category' column found for {partition_name}. Skipping category distribution plot.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nProcessing Category Distribution for: {partition_name}\")\n",
    "    category_counts = dataframe['category'].value_counts().head(10)\n",
    "    total_items = len(dataframe)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8), facecolor=BACKGROUND_COLOR)\n",
    "    bars = plt.bar(category_counts.index, category_counts.values, color=MAIN_COLOR, edgecolor='white')\n",
    "    plt.title(f\"Product Categories Distribution - {partition_name.replace('_', ' ')}\", \n",
    "              fontweight='bold', fontsize=16, color=TEXT_COLOR)\n",
    "    plt.xlabel(\"Category\", fontsize=14, color=TEXT_COLOR)\n",
    "    plt.ylabel(\"Number of Items\", fontsize=14, color=TEXT_COLOR)\n",
    "    plt.xticks(rotation=45, ha='right', color=TEXT_COLOR)\n",
    "    \n",
    "    \n",
    "    for i, bar in enumerate(bars):\n",
    "        value = category_counts.values[i]\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, value + 5, \n",
    "                 f\"{value}\", ha='center', va='bottom', \n",
    "                 fontsize=10, color=TEXT_COLOR, fontweight='bold')\n",
    "    \n",
    "    plt.grid(axis='y', alpha=0.3, color=\"#cccccc\")\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f\"category_distribution_{partition_name}.png\")\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Category distribution plot saved to: {save_path}\")\n",
    "\n",
    "# ===== GENERATE PLOTS FOR EACH PARTITION =====\n",
    "for partition_name, partition_df in partitions.items():\n",
    " \n",
    "    plot_top_products(partition_df, partition_name, plots_dir)\n",
    "    \n",
    "    plot_category_distribution(partition_df, partition_name, plots_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
